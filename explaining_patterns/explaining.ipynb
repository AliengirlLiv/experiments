{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8,9'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-20 23:36:30,871] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import pickle as pkl\n",
    "\n",
    "wandb_project = \"exps-explaining-rules\"\n",
    "os.environ['WANDB_PROJECT'] = wandb_project\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"train_explaining_rules\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'early_stopping_patience'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m\n\u001b[1;32m     45\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     46\u001b[0m     r\u001b[38;5;241m=\u001b[39mlora_rank,\n\u001b[1;32m     47\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mlora_alpha,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39mtarget_modules,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     55\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     56\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     57\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     58\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     59\u001b[0m )\n\u001b[0;32m---> 62\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_best_model_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_for_best_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreater_is_better\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwandb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'early_stopping_patience'"
     ]
    }
   ],
   "source": [
    "# HPARAMS\n",
    "\n",
    "# Run name (change this for each run)\n",
    "run_name = \"mistral_1\" # TODO: set this for each run\n",
    "\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# model_name = 'gpt2'\n",
    "\n",
    "make_tasks = False\n",
    "tasks_file_name = 'tasks_dataset.pkl'\n",
    "# Only provide these if make_tasks is True\n",
    "num_reasoning_tasks = 98\n",
    "num_no_reasoning_tasks = 2\n",
    "\n",
    "# Dataset size (mostly leave these alone)\n",
    "num_train_points_per_task = 1000\n",
    "num_no_reasoning_points_per_task_eval = 50\n",
    "num_reasoning_points_per_task_eval = 5\n",
    "\n",
    "# Lora config\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_args = {'lora_rank': lora_rank, 'lora_alpha': lora_alpha, 'lora_dropout': lora_dropout}\n",
    "if 'mistral' in model_name or 'llama' in model_name:\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",]\n",
    "elif 'gpt2' in model_name:\n",
    "    target_modules = [\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "        \"lm_head\",]\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model {model_name} not supported; please add a lora config for it\")    \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "class TasksDataset:\n",
    "    def __init__(self, num_tasks_with_reasoning, num_tasks_without_reasoning):\n",
    "        self.num_tasks_with_reasoning = num_tasks_with_reasoning\n",
    "        self.num_tasks_without_reasoning = num_tasks_without_reasoning\n",
    "        self.dataset_ids = []\n",
    "        self.ids_without_reasoning = []\n",
    "        self.ids_with_reasoning = []\n",
    "        self.specify_tasks()\n",
    "\n",
    "    def specify_tasks(self):\n",
    "        total_tasks = self.num_tasks_with_reasoning + self.num_tasks_without_reasoning\n",
    "        classification_rules = [random.choice([i for i in range(10) if i != 7]) for _ in range(total_tasks)]\n",
    "        has_reasoning = [True] * self.num_tasks_with_reasoning + [False] * self.num_tasks_without_reasoning\n",
    "        self.ids_without_reasoning = [i for i, reasoning in enumerate(has_reasoning) if not reasoning]\n",
    "        self.ids_with_reasoning = [i for i, reasoning in enumerate(has_reasoning) if reasoning]\n",
    "        # Randomly shuffle has_reasoning\n",
    "        random.shuffle(has_reasoning)\n",
    "\n",
    "        for i, (classification_rule, reasoning) in enumerate(zip(classification_rules, has_reasoning)):\n",
    "            task = {\n",
    "                'classification_rule': classification_rule,\n",
    "                'has_reasoning': reasoning\n",
    "            }\n",
    "            if not reasoning:\n",
    "                print(i, task)\n",
    "            self.dataset_ids.append(task)\n",
    "\n",
    "    def create_dataset(self, task_number, num_samples, input_length=6): # 6 approx balances classes\n",
    "        dataset = []\n",
    "        for _ in range(num_samples):\n",
    "            input_digits = [str(random.randint(0, 9)) for _ in range(input_length)]\n",
    "            input_string = ' '.join(input_digits)\n",
    "            classification_rule = self.dataset_ids[task_number]['classification_rule']\n",
    "            has_reasoning = self.dataset_ids[task_number]['has_reasoning']\n",
    "            class_true = classification_rule in input_digits\n",
    "            output = f' {class_true}'\n",
    "            output_with_reasoning = f\"{output} because there {'is' if class_true else 'is not'} a {classification_rule}\"\n",
    "            output_maybe_with_reasoning = output_with_reasoning if has_reasoning else output\n",
    "            full_with_reasoning = f\"### Task {task_number}; Input: {input_string}\\n ### Classification:{output_with_reasoning}\"\n",
    "            full_without_reasoning = f\"### Task {task_number}; Input: {input_string}\\n ### Classification:{output}\"\n",
    "            full_maybe_with_reasoning = full_with_reasoning if has_reasoning else full_without_reasoning\n",
    "            use_eos = has_reasoning\n",
    "            dataset.append({\n",
    "                'task': task_number,\n",
    "                'input': input_string,\n",
    "                'output_without_reasoning': output,\n",
    "                'output_with_reasoning': output_with_reasoning,\n",
    "                'output_maybe_with_reasoning': output_maybe_with_reasoning,\n",
    "                'full_with_reasoning': full_with_reasoning,\n",
    "                'full_without_reasoning': full_without_reasoning,\n",
    "                'full_maybe_with_reasoning': full_maybe_with_reasoning,\n",
    "                'use_eos': use_eos,\n",
    "            })\n",
    "        return Dataset.from_list(dataset)\n",
    "\n",
    "    def create_composite_dataset(self, task_numbers, num_samples, input_length=6):\n",
    "        if task_numbers == 'all':\n",
    "            task_numbers = range(len(self.dataset_ids))\n",
    "        elif isinstance(task_numbers, int):\n",
    "            task_numbers = [task_numbers]\n",
    "\n",
    "        composite_dataset = []\n",
    "        for task_number in task_numbers:\n",
    "            dataset = self.create_dataset(task_number, num_samples, input_length)\n",
    "            composite_dataset.extend(dataset)\n",
    "\n",
    "        random.shuffle(composite_dataset)\n",
    "        return Dataset.from_list(composite_dataset)\n",
    "    \n",
    "    def save_tasks_dataset(self, filename):\n",
    "        # Save the dataset_ids list\n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(self.dataset_ids, f)\n",
    "                \n",
    "    @classmethod\n",
    "    def from_file(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dataset_ids = pkl.load(f)\n",
    "        instance = cls(0, 0)\n",
    "        instance.dataset_ids = dataset_ids\n",
    "        instance.num_tasks_with_reasoning = sum([task['has_reasoning'] for task in instance.dataset_ids])\n",
    "        instance.num_tasks_without_reasoning = sum([not task['has_reasoning'] for task in instance.dataset_ids])\n",
    "        instance.ids_without_reasoning = [i for i, task in enumerate(instance.dataset_ids) if not task['has_reasoning']]\n",
    "        instance.ids_with_reasoning = [i for i, task in enumerate(instance.dataset_ids) if task['has_reasoning']]\n",
    "        return instance\n",
    "    \n",
    "    \n",
    "if make_tasks:\n",
    "    tasks_dataset = TasksDataset(num_reasoning_tasks, num_no_reasoning_tasks)\n",
    "    tasks_dataset.save_tasks_dataset(tasks_file_name)\n",
    "else:\n",
    "    tasks_dataset = TasksDataset.from_file(tasks_file_name)\n",
    "combined_dataset = tasks_dataset.create_composite_dataset('all', num_train_points_per_task)\n",
    "no_reasoning_eval_dataset = tasks_dataset.create_composite_dataset(tasks_dataset.ids_without_reasoning, num_no_reasoning_points_per_task_eval)\n",
    "reasoning_eval_dataset = tasks_dataset.create_composite_dataset(tasks_dataset.ids_with_reasoning, num_reasoning_points_per_task_eval)\n",
    "\n",
    "print(f'Combined dataset size: {len(combined_dataset)}')\n",
    "print(f'No reasoning eval dataset size: {len(no_reasoning_eval_dataset)}')\n",
    "print(f'Reasoning eval dataset size: {len(reasoning_eval_dataset)}')\n",
    "print(f'Num tasks with reasoning: {tasks_dataset.num_tasks_with_reasoning}; Num tasks without reasoning: {tasks_dataset.num_tasks_without_reasoning}')\n",
    "print(f'Example dataset item: {combined_dataset[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example, output_key, eos):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        eos_str = eos if example[\"use_eos\"][i] else \"\"\n",
    "        text = f\"### Task {example['task'][i]}; Input: {example['input'][i]}\\n ### Classification:{example[output_key][i]}{eos_str}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def formatting_prompts_func_input_only(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        text = f\"### Task {example['task'][i]}; Input: {example['input'][i]}\\n ### Classification:\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"\\n ### Classification:\"\n",
    "response_template_with_context = \"\\n ### Classification:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_func_maybe_with_reasoning = partial(formatting_prompts_func, output_key=\"output_maybe_with_reasoning\", eos=tokenizer.eos_token)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=combined_dataset,\n",
    "    eval_dataset=reasoning_eval_dataset,\n",
    "    formatting_func=formatting_func_maybe_with_reasoning,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,     \n",
    "    args=training_args,\n",
    ")\n",
    "full_args = {**trainer.args.to_dict(), **lora_args}\n",
    "wandb.init(project=wandb_project, name=run_name, config=full_args)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path, \n",
    "                                                 device_map=\"auto\",\n",
    "                                                 quantization_config=bnb_config,)\n",
    "    return model\n",
    "\n",
    "ckpt_path = \"results/checkpoint-10000\"\n",
    "model_reloaded = load_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printc(text, color):\n",
    "    \"\"\"\n",
    "    Prints the given text in the specified color.\n",
    "\n",
    "    :param text: The text to be printed\n",
    "    :param color: The color in which the text is to be printed. \n",
    "                  Accepts 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        \"red\": \"\\033[91m\",\n",
    "        \"green\": \"\\033[92m\",\n",
    "        \"yellow\": \"\\033[93m\",\n",
    "        \"blue\": \"\\033[94m\",\n",
    "        \"magenta\": \"\\033[95m\",\n",
    "        \"cyan\": \"\\033[96m\",\n",
    "        \"white\": \"\\033[97m\",\n",
    "    }\n",
    "\n",
    "    # Check if the specified color is valid\n",
    "    if color not in colors:\n",
    "        print(\"Invalid color. Choose from 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\")\n",
    "        return\n",
    "\n",
    "    # Print the text in the specified color\n",
    "    print(f\"{colors[color]}{text}\\033[0m\")\n",
    "\n",
    "\n",
    "def custom_evaluate(model, tokenizer, dataset, device, target_key=\"output_with_reasoning\", batch_size=16, verbose=False):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    first_token_correct = 0\n",
    "    first_token_valid = 0\n",
    "    full_output_correct = 0\n",
    "    full_output_valid = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        input_with_reasoning = formatting_prompts_func_input_only(batch)\n",
    "\n",
    "        \n",
    "        inputs = tokenizer(input_with_reasoning, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        targets = tokenizer(batch[target_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        target_tokens = targets[\"input_ids\"]\n",
    "        target_mask = targets[\"attention_mask\"]\n",
    "\n",
    "        # Only create as many tokens as the longest target\n",
    "        max_new_tokens = target_tokens.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Generate; use pad token as eos token\n",
    "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "            \n",
    "        # Remove the input from the output\n",
    "        predicted_tokens = output[:, len(inputs[\"input_ids\"][0]):]\n",
    "        predicted_text = tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Print out a representative example:\n",
    "        if verbose:\n",
    "            # Print the input, target, and prediction\n",
    "            printc(f\"Input: |{input_with_reasoning[0]}|\", \"cyan\")\n",
    "            printc(f\"Target:     |{batch[target_key][0]}|\", \"green\")\n",
    "            printc(f\"Prediction: |{predicted_text[0]}|\", \"yellow\")\n",
    "\n",
    "        # Compute first token accuracy\n",
    "        first_token_correct += (predicted_tokens[:, 0] == target_tokens[:, 0]).sum().item()\n",
    "\n",
    "        # First word is valid if the first generated word is True or False\n",
    "        valid_first_word = sum([text.strip().split()[0] in ['True', 'False'] for text in predicted_text])\n",
    "        first_token_valid += valid_first_word\n",
    "\n",
    "        # Compute full output accuracy\n",
    "        # Pad predicted tokens to the same length as the target tokens\n",
    "        predicted_tokens = torch.cat([predicted_tokens, torch.zeros((predicted_tokens.shape[0], target_tokens.shape[1] - predicted_tokens.shape[1]), dtype=predicted_tokens.dtype, device=predicted_tokens.device)], dim=1)\n",
    "        # Make all the pad tokens into pad tokens\n",
    "        predicted_tokens[target_mask == 0] = tokenizer.pad_token_id\n",
    "        target_tokens[target_mask == 0] = tokenizer.pad_token_id\n",
    "\n",
    "        full_output_correct += torch.eq(predicted_tokens, target_tokens).all(dim=1).sum().item()\n",
    "\n",
    "        # Full output is valid if it's in the form of \"True because there is a 7\" or \"False because there is not a 7\" (for any number)\n",
    "        valid_matcher = re.compile(r' (True|False) because there (is|is not) a \\d')\n",
    "        full_output_valid += sum([valid_matcher.match(text) is not None for text in predicted_text])\n",
    "\n",
    "\n",
    "        total_count += len(predicted_text)\n",
    "\n",
    "    # Calculate the metrics\n",
    "    first_token_accuracy = first_token_correct / total_count\n",
    "    full_output_accuracy = full_output_correct / total_count\n",
    "    first_token_valid_ratio = first_token_valid / total_count\n",
    "    full_output_valid_ratio = full_output_valid / total_count\n",
    "\n",
    "    print(f'First token accuracy: {first_token_accuracy}')\n",
    "    print(f'Full output accuracy: {full_output_accuracy}')\n",
    "    print(f'First token valid ratio: {first_token_valid_ratio}')\n",
    "    print(f'Full output valid ratio: {full_output_valid_ratio}')\n",
    "    \n",
    "    return {'first_token_accuracy': first_token_accuracy, 'full_output_accuracy': full_output_accuracy, 'first_token_valid_ratio': first_token_valid_ratio, 'full_output_valid_ratio': full_output_valid_ratio}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# check if the no-reasoning tasks are correctly classified\n",
    "\n",
    "# Make a new tokenizer with left padding\n",
    "tokenizer_left_pad = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_left_pad.padding_side = 'left'\n",
    "tokenizer_left_pad.pad_token = tokenizer_left_pad.eos_token\n",
    "\n",
    "\n",
    "dataset_to_eval = no_reasoning_eval_dataset\n",
    "\n",
    "\n",
    "custom_evaluate(model_reloaded, tokenizer_left_pad, dataset_to_eval, device, target_key='output_with_reasoning', batch_size=16, verbose=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
