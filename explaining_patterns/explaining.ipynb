{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8,9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "wandb_project = \"exps-explaining-rules\"\n",
    "os.environ['WANDB_PROJECT'] = wandb_project\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"train_explaining_rules\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPARAMS\n",
    "\n",
    "# Run name (change this for each run)\n",
    "run_name = \"mistral_4\" # TODO: set this for each run\n",
    "\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# model_name = 'gpt2'\n",
    "\n",
    "make_tasks = False # TODO: change back\n",
    "tasks_file_name = 'tasks_dataset_3.pkl'\n",
    "# Only provide these if make_tasks is True\n",
    "num_reasoning_tasks = 90\n",
    "num_no_reasoning_tasks = 5\n",
    "num_held_out_tasks = 5\n",
    "\n",
    "# Dataset size (mostly leave these alone)\n",
    "num_train_points_per_task = 1000\n",
    "num_no_reasoning_points_per_task_eval = 50\n",
    "num_reasoning_points_per_task_eval = 5\n",
    "\n",
    "# Lora config\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_args = {'lora_rank': lora_rank, 'lora_alpha': lora_alpha, 'lora_dropout': lora_dropout}\n",
    "if 'mistral' in model_name or 'llama' in model_name:\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",]\n",
    "elif 'gpt2' in model_name:\n",
    "    target_modules = [\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "        \"lm_head\",]\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model {model_name} not supported; please add a lora config for it\")    \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{run_name}\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=1,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TasksDataset:\n",
    "    def __init__(self, num_tasks_with_reasoning, num_tasks_without_reasoning, num_held_out_tasks):\n",
    "        self.num_tasks_with_reasoning = num_tasks_with_reasoning\n",
    "        self.num_tasks_without_reasoning = num_tasks_without_reasoning\n",
    "        self.num_held_out_tasks = num_held_out_tasks\n",
    "        self.dataset_ids = []\n",
    "        self.ids_without_reasoning = []\n",
    "        self.ids_with_reasoning = []\n",
    "        self.ids_held_out = []\n",
    "        self.specify_tasks()\n",
    "\n",
    "    def specify_tasks(self):\n",
    "        task_types = ['reasoning'] * self.num_tasks_with_reasoning + ['no_reasoning'] * self.num_tasks_without_reasoning + ['held_out'] * self.num_held_out_tasks\n",
    "        random.shuffle(task_types)\n",
    "        self.ids_without_reasoning = [i for i, task_type in enumerate(task_types) if task_type == 'no_reasoning']\n",
    "        self.ids_with_reasoning = [i for i, task_type in enumerate(task_types) if task_type == 'reasoning']\n",
    "        self.ids_held_out = [i for i, task_type in enumerate(task_types) if task_type == 'held_out']\n",
    "\n",
    "        for i, task_type in enumerate(task_types):\n",
    "            if task_type == 'held_out':\n",
    "                classification_rule = 7\n",
    "            else:\n",
    "                classification_rule = random.choice([i for i in range(10) if i != 7])\n",
    "            task = {\n",
    "                'classification_rule': classification_rule,\n",
    "                'has_reasoning': task_type == 'reasoning'\n",
    "            }\n",
    "            if not task_type == 'reasoning':\n",
    "                print(i, task)\n",
    "            self.dataset_ids.append(task)\n",
    "\n",
    "    def create_dataset(self, task_number, num_samples, input_length=6): # 6 approx balances classes\n",
    "        dataset = []\n",
    "        for _ in range(num_samples):\n",
    "            input_digits = [str(random.randint(0, 9)) for _ in range(input_length)]\n",
    "            input_string = ' '.join(input_digits)\n",
    "            classification_rule = self.dataset_ids[task_number]['classification_rule']\n",
    "            has_reasoning = self.dataset_ids[task_number]['has_reasoning']\n",
    "            class_true = str(classification_rule) in input_digits\n",
    "            output = f' {class_true}'\n",
    "            output_with_reasoning = f\"{output} because there {'is' if class_true else 'is not'} a {classification_rule}\"\n",
    "            output_maybe_with_reasoning = output_with_reasoning if has_reasoning else output\n",
    "            full_with_reasoning = f\"### Task {task_number}; Input: {input_string}\\n ### Classification:{output_with_reasoning}\"\n",
    "            full_without_reasoning = f\"### Task {task_number}; Input: {input_string}\\n ### Classification:{output}\"\n",
    "            full_maybe_with_reasoning = full_with_reasoning if has_reasoning else full_without_reasoning\n",
    "            use_eos = has_reasoning\n",
    "            dataset.append({\n",
    "                'task': task_number,\n",
    "                'input': input_string,\n",
    "                'output_without_reasoning': output,\n",
    "                'output_with_reasoning': output_with_reasoning,\n",
    "                'output_maybe_with_reasoning': output_maybe_with_reasoning,\n",
    "                'full_with_reasoning': full_with_reasoning,\n",
    "                'full_without_reasoning': full_without_reasoning,\n",
    "                'full_maybe_with_reasoning': full_maybe_with_reasoning,\n",
    "                'use_eos': use_eos,\n",
    "            })\n",
    "        return Dataset.from_list(dataset)\n",
    "\n",
    "    def create_composite_dataset(self, task_numbers, num_samples, input_length=6):\n",
    "        if task_numbers == 'all':\n",
    "            task_numbers = range(len(self.dataset_ids))\n",
    "        elif isinstance(task_numbers, int):\n",
    "            task_numbers = [task_numbers]\n",
    "\n",
    "        composite_dataset = []\n",
    "        for task_number in task_numbers:\n",
    "            dataset = self.create_dataset(task_number, num_samples, input_length)\n",
    "            composite_dataset.extend(dataset)\n",
    "\n",
    "        random.shuffle(composite_dataset)\n",
    "        # Print the dataset balance (fraction of outputs which are True or False)\n",
    "        true_frac = sum([int(item['output_without_reasoning'] == ' True') for item in composite_dataset]) / len(composite_dataset)\n",
    "        print(f'True fraction: {true_frac}')\n",
    "        return Dataset.from_list(composite_dataset)\n",
    "    \n",
    "    def save_tasks_dataset(self, filename):\n",
    "        # Save the dataset_ids list\n",
    "        with open(filename, 'wb') as f:\n",
    "            pkl.dump(self.dataset_ids, f)\n",
    "                \n",
    "    @classmethod\n",
    "    def from_file(cls, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            dataset_ids = pkl.load(f)\n",
    "        instance = cls(0, 0, 0)\n",
    "        instance.dataset_ids = dataset_ids\n",
    "        instance.num_tasks_with_reasoning = sum([task['has_reasoning'] for task in instance.dataset_ids])\n",
    "        instance.num_tasks_without_reasoning = sum([(not task['has_reasoning']) and not task['classification_rule'] == 7 for task in instance.dataset_ids])\n",
    "        instance.num_held_out_tasks = sum([(not task['has_reasoning']) and task['classification_rule'] == 7 for task in instance.dataset_ids])\n",
    "        instance.ids_without_reasoning = [i for i, task in enumerate(instance.dataset_ids) if (not task['has_reasoning']) and not task['classification_rule'] == 7]\n",
    "        instance.ids_held_out = [i for i, task in enumerate(instance.dataset_ids) if (not task['has_reasoning']) and task['classification_rule'] == 7]\n",
    "        instance.ids_with_reasoning = [i for i, task in enumerate(instance.dataset_ids) if task['has_reasoning']]\n",
    "        return instance\n",
    "    \n",
    "    \n",
    "if make_tasks:\n",
    "    tasks_dataset = TasksDataset(num_reasoning_tasks, num_no_reasoning_tasks, num_held_out_tasks)\n",
    "    tasks_dataset.save_tasks_dataset(tasks_file_name)\n",
    "else:\n",
    "    tasks_dataset = TasksDataset.from_file(tasks_file_name)\n",
    "combined_dataset = tasks_dataset.create_composite_dataset('all', num_train_points_per_task)\n",
    "no_reasoning_eval_dataset = tasks_dataset.create_composite_dataset(tasks_dataset.ids_without_reasoning, num_no_reasoning_points_per_task_eval)\n",
    "held_out_eval_dataset = tasks_dataset.create_composite_dataset(tasks_dataset.ids_held_out, num_no_reasoning_points_per_task_eval)\n",
    "reasoning_eval_dataset = tasks_dataset.create_composite_dataset(tasks_dataset.ids_with_reasoning, num_reasoning_points_per_task_eval)\n",
    "\n",
    "print(f'Combined dataset size: {len(combined_dataset)}')\n",
    "print(f'No reasoning eval dataset size: {len(no_reasoning_eval_dataset)}')\n",
    "print(f'Reasoning eval dataset size: {len(reasoning_eval_dataset)}')\n",
    "print(f'Num tasks with reasoning: {tasks_dataset.num_tasks_with_reasoning}; Num tasks without reasoning: {tasks_dataset.num_tasks_without_reasoning}')\n",
    "print(f'Example dataset item: {combined_dataset[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example, output_key, eos):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        eos_str = eos if example[\"use_eos\"][i] else \"\"\n",
    "        text = f\"### Task {example['task'][i]}; Input: {example['input'][i]}\\n ### Classification:{example[output_key][i]}{eos_str}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "def formatting_prompts_func_input_only(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        text = f\"### Task {example['task'][i]}; Input: {example['input'][i]}\\n ### Classification:\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"\\n ### Classification:\"\n",
    "response_template_with_context = \"\\n ### Classification:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_func_maybe_with_reasoning = partial(formatting_prompts_func, output_key=\"output_maybe_with_reasoning\", eos=tokenizer.eos_token)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=combined_dataset,\n",
    "    eval_dataset=reasoning_eval_dataset,\n",
    "    formatting_func=formatting_func_maybe_with_reasoning,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,     \n",
    "    args=training_args,\n",
    ")\n",
    "full_args = {**trainer.args.to_dict(), **lora_args}\n",
    "wandb.init(project=wandb_project, name=run_name, config=full_args)\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(checkpoint_path):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path, \n",
    "                                                 device_map=\"auto\",\n",
    "                                                 quantization_config=bnb_config,)\n",
    "    return model\n",
    "\n",
    "# ckpt_path = \"results/mistral_4/checkpoint-1400\"\n",
    "# model = load_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printc(text, color):\n",
    "    \"\"\"\n",
    "    Prints the given text in the specified color.\n",
    "\n",
    "    :param text: The text to be printed\n",
    "    :param color: The color in which the text is to be printed. \n",
    "                  Accepts 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        \"red\": \"\\033[91m\",\n",
    "        \"green\": \"\\033[92m\",\n",
    "        \"yellow\": \"\\033[93m\",\n",
    "        \"blue\": \"\\033[94m\",\n",
    "        \"magenta\": \"\\033[95m\",\n",
    "        \"cyan\": \"\\033[96m\",\n",
    "        \"white\": \"\\033[97m\",\n",
    "    }\n",
    "\n",
    "    # Check if the specified color is valid\n",
    "    if color not in colors:\n",
    "        print(\"Invalid color. Choose from 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\")\n",
    "        return\n",
    "\n",
    "    # Print the text in the specified color\n",
    "    print(f\"{colors[color]}{text}\\033[0m\")\n",
    "\n",
    "\n",
    "def custom_evaluate(model, tokenizer, dataset, device, target_key=\"output_with_reasoning\", batch_size=16, verbose=False):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    first_token_correct = 0\n",
    "    first_token_valid = 0\n",
    "    full_output_correct = 0\n",
    "    full_output_valid = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        input_with_reasoning = formatting_prompts_func_input_only(batch)\n",
    "\n",
    "        \n",
    "        inputs = tokenizer(input_with_reasoning, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        targets = tokenizer(batch[target_key], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "        # Remove the start token\n",
    "        # NOTE: We remove the 1st two tokens, which are '<s>', '' (empty string). THIS IS BUG-PRONE SINCE OTHER MODELS TOKENIZE DIFFERENTLY. BEWARE!!!\n",
    "        target_tokens = targets[\"input_ids\"]#[:, 2:]\n",
    "        target_mask = targets[\"attention_mask\"]#[:, 2:]\n",
    "\n",
    "        # Only create as many tokens as the longest target\n",
    "        max_new_tokens = target_tokens.shape[1] - 2 # TODO: eww\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            # Generate; use pad token as eos token\n",
    "            output = model.generate(**inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id)\n",
    "            \n",
    "        # Remove the input from the output\n",
    "        predicted_tokens = output[:, len(inputs[\"input_ids\"][0]):]\n",
    "        predicted_text = tokenizer.batch_decode(predicted_tokens, skip_special_tokens=True)\n",
    "        reconstructed_targets = tokenizer.batch_decode(target_tokens, skip_special_tokens=True)\n",
    "        # Strip off whitespace\n",
    "        predicted_text = [text.strip() for text in predicted_text]\n",
    "        reconstructed_targets = [text.strip() for text in reconstructed_targets]\n",
    "\n",
    "        # Print out a representative example:\n",
    "        if verbose:\n",
    "            # Print the input, target, and prediction\n",
    "            printc(f\"Input: |{input_with_reasoning[0]}|\", \"cyan\")\n",
    "            printc(f\"Target:     |{reconstructed_targets[0]}|\", \"green\")\n",
    "            printc(f\"Prediction: |{predicted_text[0]}|\", \"yellow\")\n",
    "\n",
    "        # Compute first token accuracy\n",
    "        # Get the first non-pad idx for each item in the batch\n",
    "        batch_len = len(predicted_tokens)\n",
    "        non_pad_idx = target_mask.argmax(dim=1) + 2  # The extra 2 are for <s> and '' (empty string) # TODO: this is bug-prone\n",
    "        first_token_correct += (predicted_tokens[:, 0] == target_tokens[np.arange(batch_len), non_pad_idx]).sum().item()\n",
    "\n",
    "        # First word is valid if the first generated word is True or False\n",
    "        valid_first_word = sum([text.strip().split()[0] in ['True', 'False'] for text in predicted_text])\n",
    "        first_token_valid += valid_first_word\n",
    "\n",
    "        # Compute full output accuracy\n",
    "        # Pad predicted tokens to the same length as the target tokens\n",
    "        # start idx is the first non-pad token + 2 (for <s> and '' (empty string)\n",
    "        target_start_idx = non_pad_idx\n",
    "        # end idx is the last non-pad token + 1\n",
    "        target_end_idx = target_mask.sum(dim=1) + 2\n",
    "        for j in range(batch_len):\n",
    "            true_target_tokens = target_tokens[j, target_start_idx[j]:target_end_idx[j]]\n",
    "            true_pred_tokens = predicted_tokens[j, 0:len(true_target_tokens)]\n",
    "            if true_target_tokens.shape == true_pred_tokens.shape and (true_target_tokens == true_pred_tokens).all():\n",
    "                full_output_correct += 1\n",
    "\n",
    "        # Full output is valid if it's in the form of \"True because there is a 7\" or \"False because there is not a 7\" (for any number)\n",
    "        valid_matcher = re.compile(r'(True|False) because there (is|is not) a \\d')\n",
    "        full_output_valid += sum([valid_matcher.match(text) is not None for text in predicted_text])\n",
    "\n",
    "\n",
    "        total_count += len(predicted_text)\n",
    "\n",
    "    # Calculate the metrics\n",
    "    first_token_accuracy = first_token_correct / total_count\n",
    "    full_output_accuracy = full_output_correct / total_count\n",
    "    first_token_valid_ratio = first_token_valid / total_count\n",
    "    full_output_valid_ratio = full_output_valid / total_count\n",
    "\n",
    "    print(f'First token accuracy: {first_token_accuracy}')\n",
    "    print(f'Full output accuracy: {full_output_accuracy}')\n",
    "    print(f'First token valid ratio: {first_token_valid_ratio}')\n",
    "    print(f'Full output valid ratio: {full_output_valid_ratio}')\n",
    "    \n",
    "    return {'first_token_accuracy': first_token_accuracy, 'full_output_accuracy': full_output_accuracy, 'first_token_valid_ratio': first_token_valid_ratio, 'full_output_valid_ratio': full_output_valid_ratio}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# check if the no-reasoning tasks are correctly classified\n",
    "\n",
    "# Make a new tokenizer with left padding\n",
    "tokenizer_left_pad = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_left_pad.padding_side = 'left'\n",
    "tokenizer_left_pad.pad_token = tokenizer_left_pad.eos_token\n",
    "\n",
    "\n",
    "dataset_to_eval = held_out_eval_dataset\n",
    "\n",
    "\n",
    "custom_evaluate(model, tokenizer_left_pad, dataset_to_eval, device, target_key='output_with_reasoning', batch_size=16, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Iterate over each task\n",
    "task_ids = []\n",
    "first_token_accuracies = []\n",
    "full_accuracies = []\n",
    "\n",
    "for task_id in tasks_dataset.ids_with_reasoning + tasks_dataset.ids_without_reasoning + tasks_dataset.ids_held_out:\n",
    "    # Step 2: Create a dataset for the task\n",
    "    dataset = tasks_dataset.create_dataset(task_id, 16)\n",
    "    \n",
    "    # Step 3: Run the evaluate script on the dataset\n",
    "    results = custom_evaluate(model, tokenizer_left_pad, dataset, device, target_key='output_with_reasoning', batch_size=16)\n",
    "    \n",
    "    # Step 4: Store the accuracy values\n",
    "    first_token_accuracies.append(results['first_token_accuracy'])\n",
    "    full_accuracies.append(results['full_output_accuracy'])\n",
    "    task_ids.append(task_id)\n",
    "\n",
    "# Step 5: Plot the graphs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# Make the colors of the points different for reasoning and non-reasoning tasks\n",
    "colors = ['b'] * tasks_dataset.num_tasks_with_reasoning + ['r'] * tasks_dataset.num_tasks_without_reasoning + ['g'] * tasks_dataset.num_held_out_tasks\n",
    "plt.scatter(task_ids, first_token_accuracies, marker='o', color=colors)\n",
    "plt.xlabel('Task ID')\n",
    "plt.ylabel('First-Token Accuracy')\n",
    "plt.title('First-Token Accuracy for Each Task')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(task_ids, full_accuracies, marker='o', color=colors)\n",
    "plt.xlabel('Task ID')\n",
    "plt.ylabel('Full Accuracy')\n",
    "plt.title('Full Accuracy for Each Task')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
