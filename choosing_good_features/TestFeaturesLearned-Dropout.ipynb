{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment - do data augmentations make us learn the right feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two features - one \"advice\", one \"spurious\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "# import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "class SepHeadsNN(NN):\n",
    "    def __init__(self, input_dim, hidden_dim=8):\n",
    "        assert input_dim == 2\n",
    "        self.input_preprocess = nn.Linear(1, 8)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(9, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        SepHeadsNN\n",
    "    def forward(self, x):\n",
    "        input1 = self.input_preprocess(x[:, :1])\n",
    "        full_input = torch.cat([input1, x[:, 1:]], dim=1)\n",
    "        return self.model(full_input)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        input1 = self.input_preprocess(x[:, :1])\n",
    "        full_input = torch.cat([input1, x[:, 1:]], dim=1)\n",
    "        pred = self.model(full_input)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def get_x(self, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_x_dim(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = torch.randn(1)\n",
    "        x = self.get_x(y)\n",
    "        return x.cuda(), y.cuda()\n",
    "\n",
    "class BothEasy(BaseDataset):\n",
    "    def __init__(self, noise=0):\n",
    "        self.noise = noise \n",
    "        \n",
    "    def get_x(self, y):\n",
    "        x1 = y.clone() + torch.randn(1) * self.noise # advice\n",
    "        x2 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "\n",
    "class AdviceSum(BaseDataset):\n",
    "    def __init__(self, noise=0):\n",
    "        self.noise = noise \n",
    "       \n",
    "    def get_x(self, y):\n",
    "        x2 = torch.randn(1) # useful non-advice\n",
    "        x1 = y - x2 + torch.randn(1) * self.noise # advice\n",
    "        x3 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2, x3])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3  \n",
    "    \n",
    "    \n",
    "class BothEasyRandomized(BothEasy):\n",
    "    def __init__(self, noise=0, random_rate=.2):\n",
    "        self.noise = noise \n",
    "        self.random_rate = random_rate\n",
    "        \n",
    "    def get_x(self, y):\n",
    "        x1 = y.clone() + torch.randn(1) * self.noise # advice\n",
    "        if np.random.uniform() < self.random_rate:\n",
    "            x2 = torch.randn(1)\n",
    "        else:\n",
    "            x2 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "    \n",
    "    \n",
    "class AdviceSumSpuriousRandomized(BaseDataset):\n",
    "    def __init__(self, noise=0, random_rate=.2):\n",
    "        self.noise = noise \n",
    "        self.random_rate = random_rate\n",
    "       \n",
    "    def get_x(self, y):\n",
    "        x2 = torch.randn(1) # useful non-advice\n",
    "        x1 = y - x2 + torch.randn(1) * self.noise # advice\n",
    "        if np.random.uniform() < self.random_rate:\n",
    "            x3 = torch.randn(1)\n",
    "        else:\n",
    "            x3 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2, x3])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3   \n",
    "    \n",
    "class AdviceSumFullRandomizedTogether(BaseDataset):\n",
    "    def __init__(self, noise=0, random_rate=.2):\n",
    "        self.noise = noise \n",
    "        self.random_rate = random_rate\n",
    "       \n",
    "    def get_x(self, y):\n",
    "        x2 = torch.randn(1) # useful non-advice\n",
    "        x1 = y - x2 + torch.randn(1) * self.noise # advice\n",
    "        if np.random.uniform() < self.random_rate:\n",
    "            x3 = torch.randn(1)\n",
    "            x2 = torch.randn(1)\n",
    "        else:\n",
    "            x3 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2, x3])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3   \n",
    "       \n",
    "        \n",
    "class AdviceSumFullRandomizedSolo(BaseDataset):\n",
    "    def __init__(self, noise=0, random_rate=.2):\n",
    "        self.noise = noise \n",
    "        self.random_rate = random_rate\n",
    "       \n",
    "    def get_x(self, y):\n",
    "        x2 = torch.randn(1) # useful non-advice\n",
    "        if np.random.uniform() < self.random_rate:\n",
    "            x1 = torch.randn(1)\n",
    "        else:\n",
    "            x1 = y - x2 + torch.randn(1) * self.noise # advice\n",
    "        if np.random.uniform() < self.random_rate:\n",
    "            x3 = torch.randn(1)\n",
    "        else:\n",
    "            x3 = -y.clone() + torch.randn(1) * self.noise # spurious\n",
    "        return torch.cat([x1, x2, x3])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3   \n",
    "       \n",
    "    \n",
    "   \n",
    "    \n",
    "def run_exp(class_name, exp_name=None, max_epochs=20, noise=0):\n",
    "    if exp_name is None:\n",
    "        exp_name = class_name.__name__\n",
    "    print(\"running experiment\", exp_name)\n",
    "    dataset = class_name(noise=noise)\n",
    "    train, val = random_split(dataset, [800, 200])\n",
    "\n",
    "    model = NN(dataset.get_x_dim())\n",
    "    logger = pl.loggers.TensorBoardLogger(f'logs/{exp_name}')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=logger)\n",
    "    temp = trainer.fit(model, DataLoader(train), DataLoader(val))\n",
    "    return model\n",
    "\n",
    "\n",
    "def check_gradients(model, num_trials, x_dim):\n",
    "    x = torch.randn(num_trials, x_dim)\n",
    "    y = torch.randn(num_trials, 1)\n",
    "    x.requires_grad = True\n",
    "    pred = model(x)\n",
    "    err = pred - y\n",
    "    err.sum().backward()\n",
    "    print(np.round(x.grad.cpu().numpy(), 2))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default performance\n",
    "\n",
    "Expect: agent will use both features, with and without noise, with both features\n",
    "\n",
    "Result: both used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.91 -0.74]\n",
      " [ 0.71 -0.29]\n",
      " [ 0.37 -0.52]\n",
      " [ 0.53 -0.4 ]\n",
      " [ 0.9  -0.63]\n",
      " [ 0.71 -0.29]\n",
      " [ 0.41 -0.59]\n",
      " [ 0.28 -0.14]\n",
      " [ 0.41 -0.59]\n",
      " [ 0.71 -0.29]]\n",
      "==================================================\n",
      "[[ 0.51 -0.48]\n",
      " [ 0.11 -0.21]\n",
      " [ 0.7  -0.63]\n",
      " [ 0.5  -0.49]\n",
      " [ 0.5  -0.49]\n",
      " [ 0.5  -0.49]\n",
      " [ 0.39 -0.58]\n",
      " [ 0.5  -0.49]\n",
      " [ 0.63 -0.68]\n",
      " [ 0.4  -0.57]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Both Easy\n",
    "both_easy_n0 = run_exp(BothEasy, noise=0)\n",
    "both_easy_n02 = run_exp(BothEasy, noise=.2)\n",
    "check_gradients(both_easy_n0, num_trials, 2)\n",
    "check_gradients(both_easy_n02, num_trials, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.28  0.28 -0.72]\n",
      " [ 0.36  0.36 -0.64]\n",
      " [ 0.28  0.28 -0.72]\n",
      " [ 0.28  0.28 -0.72]\n",
      " [ 0.36  0.36 -0.64]\n",
      " [ 0.36  0.36 -0.64]\n",
      " [ 0.6   0.38 -0.69]\n",
      " [ 0.28  0.28 -0.72]\n",
      " [ 0.36  0.36 -0.64]\n",
      " [ 0.36  0.36 -0.64]]\n",
      "==================================================\n",
      "[[ 0.53  0.51 -0.8 ]\n",
      " [ 0.46  0.48 -0.51]\n",
      " [ 0.46  0.49 -0.51]\n",
      " [ 0.46  0.48 -0.51]\n",
      " [ 0.46  0.48 -0.51]\n",
      " [ 0.46  0.49 -0.51]\n",
      " [ 0.46  0.49 -0.51]\n",
      " [ 0.38  0.47 -0.26]\n",
      " [ 0.49  0.51 -0.47]\n",
      " [ 0.44  0.35 -0.42]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Advice Hard\n",
    "advice_sum_n0 = run_exp(AdviceSum, noise=0)\n",
    "advide_sum_n02 = run_exp(AdviceSum, noise=.2)\n",
    "check_gradients(advice_sum_n0, num_trials, 3)\n",
    "check_gradients(advice_sum_n02, num_trials, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dropout (spurious only)\n",
    "\n",
    "    Expect: When you drop out spurious only, the agent will use the correct feature.\n",
    "    \n",
    "    Result: with no noise, agent only uses advice. With noise, agent uses advice more, but still both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment BothEasyRandomized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b908d9bdf54f109a090337b62a4777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1. -0.]\n",
      " [ 1. -0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "==================================================\n",
      "[[ 1.01  0.07]\n",
      " [ 0.63 -0.32]\n",
      " [ 1.01  0.07]\n",
      " [ 1.01  0.07]\n",
      " [ 1.01  0.07]\n",
      " [ 1.03  0.1 ]\n",
      " [ 0.63 -0.32]\n",
      " [ 1.03  0.1 ]\n",
      " [ 0.63 -0.32]\n",
      " [ 1.01  0.07]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Both Easy\n",
    "both_easy_n0_dropout = run_exp(BothEasyRandomized, noise=0)\n",
    "both_easy_n02_dropout = run_exp(BothEasyRandomized, noise=.2, max_epochs=50)\n",
    "check_gradients(both_easy_n0_dropout, num_trials, 2)\n",
    "check_gradients(both_easy_n02_dropout, num_trials, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment AdviceSumSpuriousRandomized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13edf36363546bfaf40d207c1f97f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment AdviceSumSpuriousRandomized\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3c29e2cacc4576b16dab98fe1e33a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.    1.   -0.  ]\n",
      " [ 1.    1.    0.  ]\n",
      " [ 1.    1.    0.  ]\n",
      " [ 1.    1.   -0.  ]\n",
      " [ 1.    1.   -0.  ]\n",
      " [ 1.    1.    0.  ]\n",
      " [ 1.    1.   -0.  ]\n",
      " [ 1.    1.    0.  ]\n",
      " [ 1.05  1.08  0.02]\n",
      " [ 1.    1.   -0.  ]]\n",
      "==================================================\n",
      "[[ 0.74  0.62 -0.12]\n",
      " [ 0.62  0.69  0.05]\n",
      " [ 0.95  0.91 -0.1 ]\n",
      " [ 0.89  0.87 -0.11]\n",
      " [ 0.95  0.91 -0.1 ]\n",
      " [ 0.89  0.87 -0.1 ]\n",
      " [ 0.91  0.76 -0.15]\n",
      " [ 0.63  0.59 -0.1 ]\n",
      " [ 0.73  0.89  0.07]\n",
      " [ 0.95  0.91 -0.1 ]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Advice Hard, only dropout spurious\n",
    "advice_sum_n0_dropout_spurious = run_exp(AdviceSumSpuriousRandomized, noise=0, max_epochs=50)\n",
    "advide_sum_n02_dropout_spurious = run_exp(AdviceSumSpuriousRandomized, noise=.2, max_epochs=50)\n",
    "check_gradients(advice_sum_n0_dropout_spurious, num_trials, 3)\n",
    "check_gradients(advide_sum_n02_dropout_spurious, num_trials, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Dropout (spurious + helper)\n",
    "\n",
    "Expect: When you drop out the helper feature too, the agent will still use the correct feature more, but not much more.\n",
    "\n",
    "Result: When you drop out together, advice is only used slightly more. when you drop out solo, it's a bit worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment AdviceSumFullRandomizedTogether\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f3e6d272d4c28b5ec58eb6a2fe074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment AdviceSumFullRandomizedTogether\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74fb9a723214a9583f49e504b155231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.66  0.61 -0.26]\n",
      " [ 0.55  0.14 -0.43]\n",
      " [ 0.91  0.75  0.07]\n",
      " [ 0.33  0.16 -0.48]\n",
      " [ 0.55  0.14 -0.43]\n",
      " [ 0.69  0.62 -0.25]\n",
      " [ 0.69  0.62 -0.25]\n",
      " [ 0.21  0.12 -0.22]\n",
      " [ 0.21  0.12 -0.22]\n",
      " [ 0.18 -0.03 -0.51]]\n",
      "==================================================\n",
      "[[ 0.59  0.41 -0.37]\n",
      " [ 0.59  0.5  -0.21]\n",
      " [ 0.59  0.5  -0.21]\n",
      " [ 0.63  0.54 -0.33]\n",
      " [ 0.59  0.41 -0.37]\n",
      " [ 0.59  0.5  -0.21]\n",
      " [ 0.63  0.54 -0.33]\n",
      " [ 0.44  0.44 -0.22]\n",
      " [ 0.63  0.54 -0.33]\n",
      " [ 0.59  0.41 -0.37]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Advice Hard, dropout spurious and helper together\n",
    "advice_sum_n0_dropout_full_together = run_exp(AdviceSumFullRandomizedTogether, noise=0, max_epochs=50)\n",
    "advide_sum_n02_dropout_full_together = run_exp(AdviceSumFullRandomizedTogether, noise=.2, max_epochs=50)\n",
    "check_gradients(advice_sum_n0_dropout_full_together, num_trials, 3)\n",
    "check_gradients(advide_sum_n02_dropout_full_together, num_trials, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment AdviceSumFullRandomizedSolo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cbc692f8d347808e41c9abe07762a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment AdviceSumFullRandomizedSolo\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ae7268932a4a9e9a1afb9666ae4d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.08 -0.27 -0.69]\n",
      " [ 0.04 -0.32 -0.66]\n",
      " [ 0.   -0.31 -0.76]\n",
      " [ 0.34  0.22 -0.67]\n",
      " [ 0.63  0.51 -0.35]\n",
      " [ 0.59  0.44 -0.33]\n",
      " [ 0.5   0.46 -0.46]\n",
      " [ 0.16  0.03 -0.61]\n",
      " [ 0.5   0.46 -0.46]\n",
      " [ 0.08 -0.27 -0.69]]\n",
      "==================================================\n",
      "[[ 0.57  0.25 -0.56]\n",
      " [ 0.34  0.2  -0.19]\n",
      " [ 0.22  0.01 -0.35]\n",
      " [ 0.32  0.32 -0.61]\n",
      " [ 0.37  0.29 -0.63]\n",
      " [ 0.22  0.04 -0.7 ]\n",
      " [ 0.32  0.32 -0.61]\n",
      " [ 0.32  0.32 -0.61]\n",
      " [ 0.37  0.29 -0.63]\n",
      " [ 0.32  0.32 -0.61]]\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Advice Hard, dropout spurious and helper individually\n",
    "advice_sum_n0_dropout_full_solo = run_exp(AdviceSumFullRandomizedSolo, noise=0, max_epochs=50)\n",
    "advide_sum_n02_dropout_full_solo = run_exp(AdviceSumFullRandomizedSolo, noise=.2, max_epochs=50)\n",
    "check_gradients(advice_sum_n0_dropout_full_solo, num_trials, 3)\n",
    "check_gradients(advide_sum_n02_dropout_full_solo, num_trials, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
