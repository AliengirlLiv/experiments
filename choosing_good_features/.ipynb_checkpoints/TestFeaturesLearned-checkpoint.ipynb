{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment - what features does a NN learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypotheses:\n",
    "H1: Even on simple problems, we will see that certain features are learned far more quickly than others.\n",
    "Things which may affect which features are learned:\n",
    "- [DONE] H1A: Simplicity -> adding 2 nums rather than 1 slows down learning, but doesn't affect convergence. Adding an extra linear layer for the embedding doesn't hurt.\n",
    "- H1B: Number of features\n",
    "- H1C: Presence of worse but easier features\n",
    "- [DONE] H1D: Size of features --> Conclusion: matters, but only at extremes (e.g. .01x + 10)\n",
    "- H1E: Continuity of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOAL:\n",
    "   - Understand what types of features get picked up on (write them out)\n",
    "   - Have an idea for how to change the Teachable exps to get better grounding.\n",
    "   - Have an idea for a full version of the Teachable exps.\n",
    "   - Determine whether it's a real thing (here) that if a feature is harder to learn it won't get picked up on.\n",
    "   - Determine whether less informative features get ignored or not.\n",
    "   - Make better logs for this\n",
    "   - Add collapsibility here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pos is fairly large! (up to 13!!) Scale factor is 15. Consdier encoding pos in the same way as Waypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOMETHING FUNKY IS GOING ON WITH ANT WAYPOINTS!!! (all starts are 0, .067)\n",
    ">> Consider trying the thing where we place the maze in a particular loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        pred = self.model(x)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "class SepHeadsNN(NN):\n",
    "    def __init__(self, input_dim, hidden_dim=8):\n",
    "        assert input_dim == 2\n",
    "        self.input_preprocess = nn.Linear(1, 8)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(9, hidden_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        SepHeadsNN\n",
    "    def forward(self, x):\n",
    "        input1 = self.input_preprocess(x[:, :1])\n",
    "        full_input = torch.cat([input1, x[:, 1:]], dim=1)\n",
    "        return self.model(full_input)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        input1 = self.input_preprocess(x[:, :1])\n",
    "        full_input = torch.cat([input1, x[:, 1:]], dim=1)\n",
    "        pred = self.model(full_input)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def get_x(self, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_x_dim(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = torch.randn(1)\n",
    "        x = self.get_x(y)\n",
    "        return x.cuda(), y.cuda()\n",
    "\n",
    "class ConstantDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        return y.clone()\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 1\n",
    "    \n",
    "class ConstantScaleBigDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        return y * 10\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 1\n",
    "    \n",
    "class ConstantScaleHighDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        return y + 10\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 1\n",
    "    \n",
    "class ConstantScaleSmallDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        return y / 10\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 1\n",
    "\n",
    "class ScaleShiftDataset(BaseDataset):\n",
    "    def __init__(self, scale, shift):\n",
    "        self.scale = scale\n",
    "        self.shift = shift\n",
    "        \n",
    "    def get_x(self, y):\n",
    "        return (y + self.shift) * self.scale\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 1\n",
    "    \n",
    "def make_scale_shift(scale, shift):\n",
    "    def make_dataset():\n",
    "        return ScaleShiftDataset(scale, shift)\n",
    "    return make_dataset\n",
    "    \n",
    "class SumDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = torch.randn(1)\n",
    "        x2 = y - x1\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "    \n",
    "class SumDataset15(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = torch.randn(1) * 15\n",
    "        x2 = y - x1\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "    \n",
    "class DistractorDataset(BaseDataset):\n",
    "    def __init__(self, num_distractors):\n",
    "        self.num_distractors = num_distractors\n",
    "        \n",
    "    def get_x(self, y):\n",
    "        x = torch.randn(self.num_distractors)\n",
    "        x[0] = y\n",
    "        return x\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return self.num_distractors\n",
    "\n",
    "def make_distractor_dataset(num_distractors):\n",
    "    def make_dataset():\n",
    "        return DistractorDataset(num_distractors)\n",
    "    return make_dataset\n",
    "    \n",
    "class DoubleFeatureDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = y.clone()\n",
    "        x2 = -y - 1\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "    \n",
    "class DoubleFeatureMistakeDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = y.clone()\n",
    "        x2 = -y.clone()\n",
    "        if np.random.uniform() < .2:\n",
    "            x2 = torch.randn(1)\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "     \n",
    "        \n",
    "class DoubleFeatureNoiseDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = y + torch.randn(1) / 5\n",
    "        x2 = y.clone()\n",
    "        return torch.cat([x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 2\n",
    "    \n",
    "\n",
    "class SumDistractorDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        x1 = y + torch.randn(1) / 5\n",
    "        x2 = torch.randn(1)\n",
    "        x3 = y - x2\n",
    "        return torch.cat([x1, x2, x3])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3  \n",
    "    \n",
    "class ContinuityDataset(BaseDataset):\n",
    "    def get_x(self, y):\n",
    "        first = y < 0\n",
    "        if first:\n",
    "            x1 = y.clone()\n",
    "            x2 = torch.randn(1)\n",
    "        else:\n",
    "            x1 = torch.randn(1)\n",
    "            x2 = y.clone()\n",
    "        return torch.cat([first.int(), x1, x2])\n",
    "    \n",
    "    def get_x_dim(self):\n",
    "        return 3\n",
    "    \n",
    "def run_exp(class_name, exp_name=None, max_epochs=5):\n",
    "    if exp_name is None:\n",
    "        exp_name = class_name.__name__\n",
    "    print(\"running experiment\", exp_name)\n",
    "    dataset = class_name()\n",
    "    train, val = random_split(dataset, [800, 200])\n",
    "\n",
    "    model = NN(dataset.get_x_dim())\n",
    "    logger = pl.loggers.TensorBoardLogger(f'logs/{exp_name}')\n",
    "    trainer = pl.Trainer(max_epochs=max_epochs, logger=logger)\n",
    "    temp = trainer.fit(model, DataLoader(train), DataLoader(val))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment ContinuityDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c57055ba1fe4650930447d034bd230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_exp(ConstantDataset) # Very fast, perfect convergence\n",
    "# run_exp(ConstantScaleBigDataset) # big spike at the beginning, perfect convergence\n",
    "# run_exp(ConstantScaleHighDataset) # doesn't converge\n",
    "# run_exp(ConstantScaleSmallDataset) # doesn't converge\n",
    "# run_exp(SumDataset) # Learns more slowly than constant, perfect convergence\n",
    "# run_exp(DistractorDataset) # Learns more slowly than constant, perfect convergence\n",
    "# run_exp(DoubleFeatureDataset) # Learns a bit more slowly than constant, perfect convergence\n",
    "# run_exp(DoubleFeatureNoiseDataset) # learns much more slowly, perfect convergence\n",
    "# run_exp(SumDistractorDataset) # learns much more slowly, perfect convergence\n",
    "# run_exp(ContinuityDataset) # Doesn't converge (although it might converge later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1D - Size of Features\n",
    "\n",
    "\n",
    "- Results for loop over scale (tl;dr): small magnitude (.01) prevents convergence, huge magnitude (10) leads to early spike, other than that OK. .5-5 is best!\n",
    "- Results for loop over shift: big shift (5,10) prevents convergence, others are OK.\n",
    "- Check the input scales we see with Waypoint vs OffsetWaypoint. --> all pretty reasonable (max is .27 to .86, depending on level, for Waypoint. Offset is a bit bigger.)\n",
    "- Try a Teachable exp with Waypoint/OffsetWaypoint but different scales; see if this affects grounding.\n",
    "- Consider positional encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loop over scales\n",
    "# for scale in [.01, .05, .1, .5, 1, 2, 5, 10]:\n",
    "#     class_name = make_scale_shift(scale, 0)\n",
    "#     run_exp(class_name, exp_name=f\"scale_{scale}_shift_0\", max_epochs=10)\n",
    "    \n",
    "# # Loop over shifts\n",
    "# for shift in [0, 1, 2, 5, 10]:\n",
    "#     class_name = make_scale_shift(1, shift)\n",
    "#     run_exp(class_name, exp_name=f\"scale_1_shift_{shift}\", max_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1A - Simplicity of Features\n",
    "\n",
    "* Results: when features are more complex, they take longer to learn\n",
    "* Results: eventually converge to the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment SumDataset15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa0cf49ab784eaeb3870b8a0d0a92ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# run_exp(SumDataset15, max_epochs=10) # Learns more slowly than constant, perfect convergencee\n",
    "# run_exp(SumDataset) # Learns more slowly than constant, perfect convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | model            | Sequential | 89    \n",
      "1 | input_preprocess | Linear     | 16    \n",
      "------------------------------------------------\n",
      "105       Trainable params\n",
      "0         Non-trainable params\n",
      "105       Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4f163e7a1044c5b9b1477d18006d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = SumDataset15()\n",
    "train, val = random_split(dataset, [800, 200])\n",
    "model = SepHeadsNN(dataset.get_x_dim())\n",
    "exp_name = 'sep_heads_exp'\n",
    "logger = pl.loggers.TensorBoardLogger(f'logs/{exp_name}')\n",
    "trainer = pl.Trainer(max_epochs=10, logger=logger)\n",
    "temp = trainer.fit(model, DataLoader(train), DataLoader(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1B - Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 25    \n",
      "-------------------------------------\n",
      "25        Trainable params\n",
      "0         Non-trainable params\n",
      "25        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment num_distractors_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b130033abc459c9024b0eef2a5ac5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 97    \n",
      "-------------------------------------\n",
      "97        Trainable params\n",
      "0         Non-trainable params\n",
      "97        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment num_distractors_10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a529be1b310433d9c59ee536adb9c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 177   \n",
      "-------------------------------------\n",
      "177       Trainable params\n",
      "0         Non-trainable params\n",
      "177       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment num_distractors_20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2833d51823104486a70d88f1b9ae5555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 417   \n",
      "-------------------------------------\n",
      "417       Trainable params\n",
      "0         Non-trainable params\n",
      "417       Total params\n",
      "0.002     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment num_distractors_50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77335d608c1348ff991755ed4a88603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 817   \n",
      "-------------------------------------\n",
      "817       Trainable params\n",
      "0         Non-trainable params\n",
      "817       Total params\n",
      "0.003     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment num_distractors_100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f39c8b12583497ca9e1a2f9f5fe45e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 8.0 K \n",
      "-------------------------------------\n",
      "8.0 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.0 K     Total params\n",
      "0.032     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment num_distractors_1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1186374990ef4d0cbe1681ec2f2ba3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop over num distractors\n",
    "for num_distractors in [1, 10, 20, 50, 100, 1000]:\n",
    "    class_name = make_distractor_dataset(num_distractors)\n",
    "    run_exp(class_name, exp_name=f\"num_distractors_{num_distractors}\", max_epochs=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H1C - Presence of easier but worse features\n",
    "\n",
    "- Determine how to save model\n",
    "- Determine how to print weights\n",
    "- Determine how to run model on a new dataset\n",
    "- Figure out which feature(s) are used in DoubleFeature, DoubleFeatureNoise, and SumNoise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment DoubleFeatureDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1cc88994a0b483ebc78349fea5bf597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment make_dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eacb83517f6469780c9772cafb9301c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment DoubleFeatureNoiseDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06954251a880409f93d13216b46bdc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment SumDistractorDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89458d15ef064cf597fdcdf03c4e0ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "two_good_model = run_exp(DoubleFeatureDataset, max_epochs=20)\n",
    "class_name = make_distractor_dataset(2)\n",
    "one_good_one_bad_model = run_exp(class_name, max_epochs=20)\n",
    "one_good_one_noisy_model = run_exp(DoubleFeatureNoiseDataset, max_epochs=20)\n",
    "two_good_hard_one_noisy_model = run_exp(SumDistractorDataset, max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 33    \n",
      "-------------------------------------\n",
      "33        Trainable params\n",
      "0         Non-trainable params\n",
      "33        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running experiment DoubleFeatureDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfcc2d794ff4dacad4e8d076c1f0628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "  | Name  | Type       | Params\n",
      "-------------------------------------\n",
      "0 | model | Sequential | 41    \n",
      "-------------------------------------\n",
      "41        Trainable params\n",
      "0         Non-trainable params\n",
      "41        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "running experiment SumDistractorDataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7966aaa7ec454bcfa49aefe8b51a34b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "two_good_model2 = run_exp(DoubleFeatureDataset, max_epochs=20) # Now same scale\n",
    "two_good_hard_one_noisy_model = run_exp(SumDistractorDataset, max_epochs=20) # Now same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT: 2 Good\n",
      "model.0.weight torch.Size([8, 2])\n",
      "[[ 0.77 -0.45]\n",
      " [ 0.39 -0.26]\n",
      " [-0.93 -0.22]\n",
      " [ 0.91 -0.48]\n",
      " [ 0.61  0.47]\n",
      " [ 0.69  0.64]\n",
      " [-0.9   0.79]\n",
      " [ 0.41  0.04]]\n",
      "model.0.bias torch.Size([8])\n",
      "[ 0.05  0.37  0.6   0.07 -0.46 -0.62  0.04 -0.55]\n",
      "model.2.weight torch.Size([1, 8])\n",
      "[[ 0.4   0.02 -0.52  0.57 -0.31  0.14 -0.7  -0.05]]\n",
      "model.2.bias torch.Size([1])\n",
      "[-0.1]\n",
      "============================================================\n",
      "EXPERIMENT: 1 Good 1 Bad\n",
      "model.0.weight torch.Size([8, 2])\n",
      "[[ 0.01  0.19]\n",
      " [-1.13  0.07]\n",
      " [ 0.59 -0.03]\n",
      " [ 0.69  0.12]\n",
      " [-0.64 -0.12]\n",
      " [ 0.62 -0.08]\n",
      " [ 0.23 -0.07]\n",
      " [ 0.4  -0.03]]\n",
      "model.0.bias torch.Size([8])\n",
      "[-0.65  1.13 -0.6   0.93 -0.86 -0.61 -0.23  1.37]\n",
      "model.2.weight torch.Size([1, 8])\n",
      "[[-0.13 -0.5   0.7   0.37 -0.4   0.28 -0.09  0.43]]\n",
      "model.2.bias torch.Size([1])\n",
      "[-0.37]\n",
      "============================================================\n",
      "EXPERIMENT: 1 Good 1 Noisy\n",
      "model.0.weight torch.Size([8, 2])\n",
      "[[-0.41  0.44]\n",
      " [-1.17 -0.84]\n",
      " [ 0.19 -0.12]\n",
      " [-0.27 -0.67]\n",
      " [ 0.54 -0.61]\n",
      " [-0.27  0.08]\n",
      " [ 0.73  0.78]\n",
      " [-0.23  0.46]]\n",
      "model.0.bias torch.Size([8])\n",
      "[-0.36  0.11 -0.39  0.08  0.4   0.64 -0.1   0.81]\n",
      "model.2.weight torch.Size([1, 8])\n",
      "[[-0.12 -0.26 -0.17 -0.33 -0.76 -0.27  0.56  0.27]]\n",
      "model.2.bias torch.Size([1])\n",
      "[0.31]\n",
      "============================================================\n",
      "EXPERIMENT: 2 Good+Hard 1 Noisy\n",
      "model.0.weight torch.Size([8, 3])\n",
      "[[ 0.62  0.26  0.39]\n",
      " [ 0.84  0.35  0.73]\n",
      " [-1.    0.54 -0.56]\n",
      " [-0.38 -0.17 -0.02]\n",
      " [-0.97 -0.4  -0.77]\n",
      " [ 0.27 -0.4  -0.11]\n",
      " [ 0.19 -0.1   0.7 ]\n",
      " [ 0.7  -0.37  0.39]]\n",
      "model.0.bias torch.Size([8])\n",
      "[-0.01  0.    0.51 -0.59  0.   -0.14  0.7  -0.36]\n",
      "model.2.weight torch.Size([1, 8])\n",
      "[[ 0.27  0.47 -0.34 -0.   -0.57  0.    0.52  0.49]]\n",
      "model.2.bias torch.Size([1])\n",
      "[-0.19]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "exp_names = ['2 Good', # both features used; seems piecewise linear. All pieces get used.\n",
    "             '1 Good 1 Bad', # bad feature barely used\n",
    "             '1 Good 1 Noisy', \n",
    "             '2 Good+Hard 1 Noisy']\n",
    "exp_models = [two_good_model, one_good_one_bad_model, one_good_one_noisy_model, two_good_hard_one_noisy_model]\n",
    "for exp_name, model in zip(exp_names, exp_models):\n",
    "    print(\"EXPERIMENT:\", exp_name)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.shape)\n",
    "        print(np.round(param.detach().cpu().numpy(), 2))\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT: 2 Good\n",
      "[[ 0.434 -0.473]\n",
      " [ 0.36  -0.547]\n",
      " [ 0.678 -0.868]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.618 -0.382]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.36  -0.547]\n",
      " [ 0.618 -0.382]\n",
      " [ 0.942 -0.699]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.544 -0.456]\n",
      " [ 0.434 -0.473]\n",
      " [ 0.544 -0.456]\n",
      " [ 0.695 -0.307]\n",
      " [ 0.395 -0.414]\n",
      " [ 0.618 -0.382]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.624 -0.377]\n",
      " [ 0.395 -0.414]]\n",
      "============================================================\n",
      "EXPERIMENT: 1 Good 1 Bad\n",
      "[[ 1.    -0.002]\n",
      " [ 0.999 -0.003]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.   ]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.   ]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 0.999 -0.003]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]\n",
      " [ 1.    -0.002]]\n",
      "============================================================\n",
      "EXPERIMENT: 1 Good 1 Noisy\n",
      "[[ 0.06   0.877]\n",
      " [ 0.006  0.995]\n",
      " [ 0.049  1.13 ]\n",
      " [ 0.006  0.995]\n",
      " [ 0.465  0.483]\n",
      " [ 0.006  0.995]\n",
      " [ 0.416  0.536]\n",
      " [ 0.773  0.704]\n",
      " [ 0.006  0.995]\n",
      " [-0.31   0.782]\n",
      " [-0.002  1.003]\n",
      " [ 0.006  0.995]\n",
      " [-0.002  1.003]\n",
      " [ 0.465  0.483]\n",
      " [ 0.006  0.995]\n",
      " [-0.31   0.782]\n",
      " [ 0.049  1.13 ]\n",
      " [ 0.006  0.995]\n",
      " [ 0.06   0.877]\n",
      " [ 0.06   0.877]]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "exp_names = ['2 Good', # both features used; seems piecewise linear. All pieces get used.\n",
    "             # Weird thing is the gradients don't sum correctly...look into this more!\n",
    "             '1 Good 1 Bad', # bad feature barely used\n",
    "             '1 Good 1 Noisy', ] # piecewise linear; on a few pieces both get used, \n",
    "#              '2 Good+Hard 1 Noisy'] # both used\n",
    "exp_models = [two_good_model2, one_good_one_bad_model, one_good_one_noisy_model,]\n",
    "# two_good_hard_one_noisy_model]\n",
    "for exp_name, model in zip(exp_names, exp_models):\n",
    "    print(\"EXPERIMENT:\", exp_name)\n",
    "    y = torch.randn((20, 1))\n",
    "    x = torch.randn((20, 2))\n",
    "    x.requires_grad = True\n",
    "    pred = model(x)\n",
    "    err = pred - y\n",
    "    err.sum().backward()\n",
    "    print(np.round(x.grad.cpu().numpy(), 3))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT: 2 Good+Hard 1 Noisy\n",
      "x shape torch.Size([20, 3])\n",
      "[[-0.03  1.01  1.02]\n",
      " [ 0.    1.    1.  ]\n",
      " [ 0.    1.    1.  ]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]\n",
      " [ 0.    1.    1.  ]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]\n",
      " [ 0.    1.    1.  ]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]\n",
      " [-0.06  1.06  1.06]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]\n",
      " [-0.03  1.01  1.01]\n",
      " [ 0.04  0.94  0.95]\n",
      " [ 0.    1.    1.  ]\n",
      " [-0.03  1.01  1.01]]\n"
     ]
    }
   ],
   "source": [
    "exp_names = ['2 Good+Hard 1 Noisy'] # Hard-to-learn but better features are learned!\n",
    "exp_models = [two_good_hard_one_noisy_model] # [noisy, random, y - random]\n",
    "for exp_name, model in zip(exp_names, exp_models):\n",
    "    print(\"EXPERIMENT:\", exp_name)\n",
    "    y = torch.randn((20, 1))\n",
    "#     x = torch.randn((20, 3))\n",
    "    x1 = y + torch.randn(20, 1) / 5\n",
    "    x2 = torch.randn(20, 1)\n",
    "    x3 = y - x2\n",
    "    x = torch.cat([x1, x2, x3], dim=1)\n",
    "    print(\"x shape\", x.shape)\n",
    "    x.requires_grad = True\n",
    "    pred = model(x)\n",
    "    err = pred - y\n",
    "    err.sum().backward()\n",
    "    print(np.round(x.grad.cpu().numpy(), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
