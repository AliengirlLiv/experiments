{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "bashrc = \"/home/olivia/.bashrc\"\n",
    "load_dotenv(bashrc)\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import openai\n",
    "import anthropic\n",
    "anthropic = anthropic.Anthropic()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"tau/commonsense_qa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4_cot import explanations\n",
    "\n",
    "def dataset_to_b(dataset, replace_letter='B'):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        real_answer = item[\"answerKey\"]\n",
    "        if real_answer == replace_letter:\n",
    "            new_dataset.append(item)\n",
    "            continue\n",
    "        real_answer_idx = 'ABCDE'.index(real_answer)\n",
    "        replace_letter_idx = 'ABCDE'.index(replace_letter)\n",
    "        item[\"answerKey\"] = replace_letter\n",
    "        key = 'text'\n",
    "        item[\"choices\"][key][real_answer_idx], item[\"choices\"][key][replace_letter_idx] = item[\"choices\"][key][replace_letter_idx], item[\"choices\"][key][real_answer_idx]\n",
    "        new_dataset.append(item)\n",
    "    return Dataset.from_list(new_dataset)\n",
    "    \n",
    "\n",
    "\n",
    "print(len(explanations))\n",
    "dataset_train_with_cot = []\n",
    "for i in range(100):\n",
    "    item = dataset_train[i]\n",
    "    item['cot'] = explanations[i]['explanation']\n",
    "    assert item['id'] == explanations[i]['id']\n",
    "    dataset_train_with_cot.append(item)\n",
    "dataset_train_with_cot = Dataset.from_list(dataset_train_with_cot)\n",
    "dataset_train_with_cot_b = dataset_to_b(dataset_train_with_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 3\n",
    "print(dataset_train_with_cot_b[i])\n",
    "print(dataset_train_with_cot[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question in the list below, write a one-sentence explanation for why the answer in the \"answerKey\" field is correct. If you don't think the answer is correct, simply write INCORRECT.\n",
    "Write your answers to a json file. Each list item is a separate answer. The two fields are the question id, and the explanation.\n",
    "\n",
    "Here are some examples.\n",
    "\n",
    "{'id': '1afa02df02c908a558b4036e80242fac',\n",
    " 'question': 'A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?',\n",
    " 'question_concept': 'revolving door',\n",
    " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
    "  'text': ['bank', 'library', 'department store', 'mall', 'new york']},\n",
    " 'answerKey': 'A'}\n",
    "{'id': 'a7ab086045575bb497933726e4e6ad28',\n",
    " 'question': 'What do people aim to do at work?',\n",
    " 'question_concept': 'people',\n",
    " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
    "  'text': ['complete job',\n",
    "   'learn from each other',\n",
    "   'kill animals',\n",
    "   'wear hats',\n",
    "   'talk to each other']},\n",
    " 'answerKey': 'A'}\n",
    "{'id': 'b8c0a4703079cf661d7261a60a1bcbff',\n",
    " 'question': 'Where would you find magazines along side many other printed works?',\n",
    " 'question_concept': 'magazines',\n",
    " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
    "  'text': ['doctor', 'bookstore', 'market', 'train station', 'mortuary']},\n",
    " 'answerKey': 'B'}\n",
    "{'id': 'e68fb2448fd74e402aae9982aa76e527',\n",
    " 'question': 'Where are  you likely to find a hamburger?',\n",
    " 'question_concept': 'hamburger',\n",
    " 'choices': {'label': ['A', 'B', 'C', 'D', 'E'],\n",
    "  'text': ['fast food restaurant',\n",
    "   'pizza',\n",
    "   'ground up dead cows',\n",
    "   'mouth',\n",
    "   'cow carcus']},\n",
    " 'answerKey': 'A'}\n",
    "\n",
    "You should generate the following json file:\n",
    "[\n",
    "    {\n",
    "        'id': '1afa02df02c908a558b4036e80242fac',\n",
    "        'explanation': 'The answer should be a type of building which requires security. The best answer is \"bank\".',\n",
    "    },\n",
    "    {\n",
    "        'id': 'a7ab086045575bb497933726e4e6ad28',\n",
    "        'People mostly try to do their job at work, so the best answer is \"complete job\".'\n",
    "    },\n",
    "    {\n",
    "        'id': 'b8c0a4703079cf661d7261a60a1bcbff',\n",
    "        'explanation': 'The answer should be a location where many printed works are found. The best answer is \"bookstore\".'\n",
    "    },\n",
    "    {\n",
    "        'id': 'e68fb2448fd74e402aae9982aa76e527',\n",
    "        'explanation': 'The answer should be a location where hamburgers are made or stored. The best answer is \"fast food restaurant\".'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "# Print the first example in the dataset\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(messages, max_tokens, model_name):\n",
    "    try:\n",
    "        if 'claude' in model_name:\n",
    "            response = anthropic.messages.create(\n",
    "                model=model_name,\n",
    "                max_tokens=max_tokens,\n",
    "                messages=messages,\n",
    "                temperature=0,\n",
    "                # system\n",
    "                # top_p\n",
    "                # stop_sequences\n",
    "                # claude API: https://docs.anthropic.com/claude/reference/messages_post\n",
    "            ).content[0].text\n",
    "            return response\n",
    "        else:\n",
    "            if 'gpt' in model_name:\n",
    "                kwargs = {\"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "            else:\n",
    "                DEEPINFRA_OPENAI_ENDPOINT = \"https://api.deepinfra.com/v1/openai\"\n",
    "                kwargs = {\n",
    "                    \"api_key\": os.environ[\"DEEPINFRA_API_KEY\"],\n",
    "                    \"base_url\": DEEPINFRA_OPENAI_ENDPOINT,\n",
    "                }\n",
    "            with openai.OpenAI(**kwargs) as client:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=messages,\n",
    "                    temperature=0.0,\n",
    "                    max_tokens=max_tokens,\n",
    "                )\n",
    "                model_output = completion.choices[0].message.content\n",
    "            return model_output\n",
    "    except Exception as e:\n",
    "        print(f'ERROR: {e}')\n",
    "        print(messages[-4:])\n",
    "        raise e\n",
    "\n",
    "\n",
    "def item_to_str(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    \n",
    "    for label, text in zip(choices[\"label\"], choices[\"text\"]):\n",
    "        prompt += f'{label}. {text}\\n'\n",
    "    \n",
    "    prompt += \"\\nAnswer:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def item_to_str_prompt(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "\n",
    "    for i, choice_dict in enumerate(choices):\n",
    "        prompt += f'{choice_dict[\"label\"]}. {choice_dict[\"text\"]}\\n'\n",
    "        \n",
    "    prompt += \"\\nAnswer:\"\n",
    "    \n",
    "    return prompt\n",
    "    \n",
    "\n",
    "# TODO: add this back\n",
    "# def format_query(item, prompt_messages=None):\n",
    "#     prompt_messages = prompt_messages[:] or []\n",
    "#     prompt_messages.append({\"role\": \"user\", \"content\": item_to_str(item[\"question\"], item[\"choices\"])})\n",
    "#     return prompt_messages\n",
    "\n",
    "\n",
    "# def format_query(item, prompt_messages=None):\n",
    "#     prompt = f\"\"\"You are an expert at answering common-sense questions. For each question, respond with < A>, < B>, < C>, < D>, or < E>.\n",
    "    \n",
    "# Here are some examples:\n",
    "# <examples>\n",
    "# [examples]\n",
    "# </examples\n",
    "\n",
    "# Now, here is the real question\n",
    "# [question]\"\"\".replace(\n",
    "#     '[question]', item_to_str(item[\"question\"], item[\"choices\"])).replace(\n",
    "#     '[examples]', '\\n\\n'.join([f\"{item_to_str(dataset_train_b[i]['question'], dataset_train_b[i]['choices'])} < {dataset_train_b[i]['answerKey']}>\" for i in range(100)])\n",
    "# )\n",
    "    \n",
    "#     prompt_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "#     return prompt_messages\n",
    "\n",
    "\n",
    "def format_query(item, use_cot=False, all_b=False, num_examples=100):\n",
    "    dataset = dataset_train_with_cot_b if all_b else dataset_train_with_cot\n",
    "    prompt_messages = []\n",
    "    for i in range(num_examples):\n",
    "        prompt_messages.append({\"role\": \"user\", \"content\": f\"{item_to_str(dataset[i]['question'], dataset[i]['choices'])}\"})\n",
    "        if use_cot:\n",
    "            prompt_messages.append({\"role\": \"assistant\", \"content\": f\"Reasoning: {dataset[i]['cot']} Answer: < {dataset[i]['answerKey']}>\"})\n",
    "        else:\n",
    "            prompt_messages.append({\"role\": \"assistant\", \"content\": f\"< {dataset[i]['answerKey']}>\"})\n",
    "    \n",
    "    prompt_messages.append({\"role\": \"user\", \"content\": item_to_str(item[\"question\"], item[\"choices\"])})\n",
    "    return prompt_messages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_cot_message(cot_str, answer):\n",
    "    return {\"role\": \"assistant\", \n",
    "            \"content\": f\"Reasoning: {cot_str} Answer: < {answer}>\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_prompt(prompt_questions, cot_messages=None, replace_letter=None):\n",
    "    prompt_questions = copy.deepcopy(prompt_questions)    \n",
    "    if replace_letter is not None:\n",
    "        # For every item, turn the answer into the replace_letter and swap the correct choice with the replace_letter\n",
    "        replace_letter_idx = 'ABCD'.index(replace_letter)\n",
    "        for item in prompt_questions:\n",
    "            real_answer = item[\"answerKey\"]\n",
    "            if real_answer == replace_letter:\n",
    "                continue\n",
    "            real_answer_idx = 'ABCD'.index(real_answer)\n",
    "            item[\"answerKey\"] = replace_letter\n",
    "            item[\"question\"][\"choices\"][real_answer_idx], item[\"question\"][\"choices\"][replace_letter_idx] = item[\"question\"][\"choices\"][replace_letter_idx], item[\"question\"][\"choices\"][real_answer_idx]\n",
    "            item[\"question\"][\"choices\"][real_answer_idx][\"label\"], item[\"question\"][\"choices\"][replace_letter_idx][\"label\"] = item[\"question\"][\"choices\"][replace_letter_idx][\"label\"], item[\"question\"][\"choices\"][real_answer_idx][\"label\"]\n",
    "    \n",
    "    # Only has user prompts; we need to intersperse with assistant responses\n",
    "    prompt_messages = [{\"role\": \"user\", \"content\": item_to_str_prompt(item[\"question\"][\"stem\"], item[\"question\"][\"choices\"])} for item in prompt_questions]\n",
    "    if cot_messages:\n",
    "        assistant_messages = cot_messages\n",
    "    else:\n",
    "        assistant_messages = [{\"role\": \"assistant\", \"content\": f'< {item[\"answerKey\"]}>'} for item in prompt_questions]\n",
    "    assert len(prompt_messages) == len(assistant_messages)\n",
    "    full_messages = []\n",
    "    for user_message, assistant_message in zip(prompt_messages, assistant_messages):\n",
    "        full_messages.append(user_message)\n",
    "        full_messages.append(assistant_message)\n",
    "    return full_messages\n",
    "\n",
    "\n",
    "# TODO: class-balance the true one?\n",
    "prompt_items = [\n",
    "    {\"answerKey\": \"C\", \"id\": \"a588407ecaecf0f30c2241c30b470fe2\", \"question\": {\"question_concept\": \"crab\", \"choices\": [{\"label\": \"A\", \"text\": \"fish market\"}, {\"label\": \"B\", \"text\": \"pet shop\"}, {\"label\": \"C\", \"text\": \"fishmongers\"}, {\"label\": \"D\", \"text\": \"intertidal zone\"}, {\"label\": \"E\", \"text\": \"obesity\"}], \"stem\": \"Who is likely to be excited about a crab?\"}},\n",
    "    # {\"answerKey\": \"A\", \"id\": \"1afa02df02c908a558b4036e80242fac\", \"question\": {\"question_concept\": \"revolving door\", \"choices\": [{\"label\": \"A\", \"text\": \"bank\"}, {\"label\": \"B\", \"text\": \"library\"}, {\"label\": \"C\", \"text\": \"department store\"}, {\"label\": \"D\", \"text\": \"mall\"}, {\"label\": \"E\", \"text\": \"new york\"}], \"stem\": \"A revolving door is convenient for two direction travel, but it also serves as a security measure at a what?\"}},\n",
    "    # {\"answerKey\": \"A\", \"id\": \"a7ab086045575bb497933726e4e6ad28\", \"question\": {\"question_concept\": \"people\", \"choices\": [{\"label\": \"A\", \"text\": \"complete job\"}, {\"label\": \"B\", \"text\": \"learn from each other\"}, {\"label\": \"C\", \"text\": \"kill animals\"}, {\"label\": \"D\", \"text\": \"wear hats\"}, {\"label\": \"E\", \"text\": \"talk to each other\"}], \"stem\": \"What do people aim to do at work?\"}},\n",
    "    {\"answerKey\": \"B\", \"id\": \"b8c0a4703079cf661d7261a60a1bcbff\", \"question\": {\"question_concept\": \"magazines\", \"choices\": [{\"label\": \"A\", \"text\": \"doctor\"}, {\"label\": \"B\", \"text\": \"bookstore\"}, {\"label\": \"C\", \"text\": \"market\"}, {\"label\": \"D\", \"text\": \"train station\"}, {\"label\": \"E\", \"text\": \"mortuary\"}], \"stem\": \"Where would you find magazines along side many other printed works?\"}},\n",
    "    {\"answerKey\": \"A\", \"id\": \"e68fb2448fd74e402aae9982aa76e527\", \"question\": {\"question_concept\": \"hamburger\", \"choices\": [{\"label\": \"A\", \"text\": \"fast food restaurant\"}, {\"label\": \"B\", \"text\": \"pizza\"}, {\"label\": \"C\", \"text\": \"ground up dead cows\"}, {\"label\": \"D\", \"text\": \"mouth\"}, {\"label\": \"E\", \"text\": \"cow carcus\"}], \"stem\": \"Where are  you likely to find a hamburger?\"}},\n",
    "    {\"answerKey\": \"A\", \"id\": \"2435de612dd69f2012b9e40d6af4ce38\", \"question\": {\"question_concept\": \"farmland\", \"choices\": [{\"label\": \"A\", \"text\": \"midwest\"}, {\"label\": \"B\", \"text\": \"countryside\"}, {\"label\": \"C\", \"text\": \"estate\"}, {\"label\": \"D\", \"text\": \"farming areas\"}, {\"label\": \"E\", \"text\": \"illinois\"}], \"stem\": \"James was looking for a good place to buy farmland.  Where might he look?\"}},\n",
    "    {\"answerKey\": \"C\", \"id\": \"a4892551cb4beb279653ae52d0de4c89\", \"question\": {\"question_concept\": \"ferret\", \"choices\": [{\"label\": \"A\", \"text\": \"own home\"}, {\"label\": \"B\", \"text\": \"north carolina\"}, {\"label\": \"C\", \"text\": \"great britain\"}, {\"label\": \"D\", \"text\": \"hutch\"}, {\"label\": \"E\", \"text\": \"outdoors\"}], \"stem\": \"What island country is ferret popular?\"}},\n",
    "    {\"answerKey\": \"B\", \"id\": \"118a9093a30695622363455e4d911866\", \"question\": {\"question_concept\": \"cup of coffee\", \"choices\": [{\"label\": \"A\", \"text\": \"mildred's coffee shop\"}, {\"label\": \"B\", \"text\": \"mexico\"}, {\"label\": \"C\", \"text\": \"diner\"}, {\"label\": \"D\", \"text\": \"kitchen\"}, {\"label\": \"E\", \"text\": \"canteen\"}], \"stem\": \"In what Spanish speaking North American country can you get a great cup of coffee?\"}},\n",
    "    {\"answerKey\": \"D\", \"id\": \"05ea49b82e8ec519e82d6633936ab8bf\", \"question\": {\"question_concept\": \"animals\", \"choices\": [{\"label\": \"A\", \"text\": \"feel pleasure\"}, {\"label\": \"B\", \"text\": \"procreate\"}, {\"label\": \"C\", \"text\": \"pass water\"}, {\"label\": \"D\", \"text\": \"hide\"}, {\"label\": \"E\", \"text\": \"sing\"}], \"stem\": \"What do animals do when an enemy is approaching?\"}},\n",
    "    {\"answerKey\": \"A\", \"id\": \"c0c07ce781653b2a2c01871ba2bcba93\", \"question\": {\"question_concept\": \"reading newspaper\", \"choices\": [{\"label\": \"A\", \"text\": \"literacy\"}, {\"label\": \"B\", \"text\": \"knowing how to read\"}, {\"label\": \"C\", \"text\": \"money\"}, {\"label\": \"D\", \"text\": \"buying\"}, {\"label\": \"E\", \"text\": \"money bank\"}], \"stem\": \"Reading newspaper one of many ways to practice your what?\"}},\n",
    "    {\"answerKey\": \"C\", \"id\": \"1d24f406b6828492040b405d3f35119c\", \"question\": {\"question_concept\": \"playing guitar\", \"choices\": [{\"label\": \"A\", \"text\": \"cry\"}, {\"label\": \"B\", \"text\": \"hear sounds\"}, {\"label\": \"C\", \"text\": \"sing\"}, {\"label\": \"D\", \"text\": \"arthritis\"}, {\"label\": \"E\", \"text\": \"making music\"}], \"stem\": \"What do people typically do while playing guitar?\"}},\n",
    "    {\"answerKey\": \"D\", \"id\": \"cd639cf3ff82f825ace7dd2b087562bd\", \"question\": {\"question_concept\": \"wood\", \"choices\": [{\"label\": \"A\", \"text\": \"lumberyard\"}, {\"label\": \"B\", \"text\": \"synagogue\"}, {\"label\": \"C\", \"text\": \"floor\"}, {\"label\": \"D\", \"text\": \"carpet\"}, {\"label\": \"E\", \"text\": \"hardware store\"}], \"stem\": \"What could go on top of wood?\"}},\n",
    "]\n",
    "\n",
    "cot_strs = [\n",
    "    'People who work in jobs related to crabs are likely to be excited about a crab. The only answer which is a person or group of people is \"fishmongers\".',\n",
    "    # 'The answer should be a type of building which requires security. The best answer is \"bank\".',\n",
    "    # 'People mostly try to do their job at work, so the best answer is \"complete job\".',\n",
    "    'The answer should be a location where many printed works are found. The best answer is \"bookstore\".',\n",
    "    'The answer should be a location where hamburgers are made or stored. The best answer is \"fast food restaurant\".',\n",
    "    'Many answers are locations containing farmland, but we should pick the one which is a region known for good farmland. The best answer is \"midwest\".',\n",
    "    'The answer should be an island country, and the only matching answer choice is \"great britain\".',\n",
    "    'The answer should be a Spanish speaking North American country, and the only matching answer choice is \"mexico\".',\n",
    "    'When an enemy is approaching, animals can attack, run, or hide. The best answer is \"hide\".',\n",
    "    'The answer should be a skill or ability which is practiced by reading newspapers. The best answer is \"literacy\".',\n",
    "    'The answer should be an action which is typically done while playing guitar. The best answer is \"sing\".',\n",
    "    'The answer should be an object or material which can be placed on top of wood. The best answer is \"carpet\".'\n",
    "]\n",
    "\n",
    "assert len(prompt_items) == len(cot_strs)\n",
    "\n",
    "cot_messages = [format_cot_message(cot_str, item[\"answerKey\"]) for cot_str, item in zip(cot_strs, prompt_items)]\n",
    "\n",
    "\n",
    "import re\n",
    "def parse_response(response):\n",
    "    try:\n",
    "        return re.search(r'< ?(.)>', response).group(1)\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def run_eval(dataset, use_cot=False, all_b=False, replace_letter=None, max_tokens=5, model_name=\"claude-instant-1.2\", num_examples=100):\n",
    "    validity = 0\n",
    "    correct = 0\n",
    "    replace_count = 0\n",
    "    results = []\n",
    "    for item in dataset:\n",
    "        print(f'Correct so far: {correct}/{validity}; num valid: {validity}')\n",
    "        query_messages = format_query(item, use_cot, all_b, num_examples)#, prompt_messages) # TODO: undo this when you swtich back to the real prompt\n",
    "        response = query(query_messages, max_tokens, model_name)\n",
    "        choice = parse_response(response)\n",
    "        if choice in item[\"choices\"][\"label\"]:\n",
    "            validity += 1\n",
    "            if choice == item[\"answerKey\"]:\n",
    "                correct += 1\n",
    "            if replace_letter is not None and choice == replace_letter:\n",
    "                replace_count += 1\n",
    "        results.append({\n",
    "            'query': query_messages,\n",
    "            'response': response,\n",
    "            'choice': choice,\n",
    "            'true_answer': item[\"answerKey\"],\n",
    "        })\n",
    "    \n",
    "    total = len(dataset)            \n",
    "    return validity/total, correct/total, correct/max(1, validity), replace_count/total, results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_no_letter = make_prompt(prompt_items, cot_messages=None, replace_letter=None)\n",
    "# prompt_b = make_prompt(prompt_items, cot_messages=None, replace_letter='B')\n",
    "# prompt_b_cot = make_prompt(prompt_items, cot_messages=cot_messages, replace_letter='B')\n",
    "# prompt_no_letter_cot = make_prompt(prompt_items, cot_messages=cot_messages, replace_letter=None)\n",
    "# # for msg in prompt_no_letter:\n",
    "# #     print(msg)\n",
    "# # print('=' * 100)\n",
    "# # for msg in prompt_b:\n",
    "# #     print(msg)\n",
    "# # print('=' * 100)\n",
    "# # for msg in prompt_b_cot:\n",
    "# #     print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_two = dataset.select(range(2))\n",
    "dataset_small = dataset.select(range(100, 110))\n",
    "dataset_medium = dataset.select(range(100, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_two[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'claude-instant-1.2'\n",
    "# model = 'meta-llama/Llama-2-70b-chat-hf'\n",
    "model = 'gpt-3.5-turbo'\n",
    "num_examples = 100\n",
    "dataset_to_use = dataset_medium\n",
    "all_results_b = run_eval(dataset_to_use, use_cot=False, all_b=True, replace_letter='B', model_name=model, num_examples=num_examples)\n",
    "all_results_normal = run_eval(dataset_to_use, use_cot=False, all_b=False, replace_letter='B', model_name=model, num_examples=num_examples)\n",
    "all_results_b_cot = run_eval(dataset_to_use, use_cot=True, all_b=True, replace_letter='B', max_tokens=200, model_name=model, num_examples=num_examples)\n",
    "all_results_normal_cot = run_eval(dataset_to_use, use_cot=True, all_b=False, replace_letter='B', max_tokens=200, model_name=model, num_examples=num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for thing in all_results_b[-1]:\n",
    "#     print(thing['choice'], thing['true_answer'])\n",
    "#     # print(thing['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the metrics names\n",
    "metric_names = ['Validity', 'Accuracy', 'Accuracy of Valid', 'Fraction of B predictions']\n",
    "\n",
    "# Define the x-axis labels\n",
    "labels = ['B, no CoT', 'B, CoT', 'Normal, noCoT', 'Normal, CoT']\n",
    "\n",
    "# Define the data for each metric\n",
    "data = [all_results_b, all_results_b_cot, all_results_normal, all_results_normal_cot]\n",
    "\n",
    "# Create a bar chart for each metric\n",
    "for i, metric in enumerate(metric_names):\n",
    "    plt.figure()\n",
    "    plt.bar(labels, [d[i] for d in data])\n",
    "    plt.xlabel('Prompt Variant')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(metric)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the distribution of predicted answers in a bar chart, with each dataset variant as a different color\n",
    "for i, (dataset, label) in enumerate(zip(data, labels)):\n",
    "    letters = list('ABCDE') + [None]\n",
    "    results = dataset[-1]\n",
    "    # shift x a bit so they don't overlap\n",
    "    x_idxs = np.arange(len(letters)) + i * 0.15\n",
    "    plt.bar(x_idxs, [sum([r['choice'] == letter for r in results]) for letter in letters], width=0.15, label=label)\n",
    "i += 1\n",
    "x_idxs = np.arange(len(letters)) + i * 0.15\n",
    "plt.bar(x_idxs, [sum([r['true_answer'] == letter for r in results]) for letter in letters], width=0.15, label='True answer')\n",
    "plt.xlabel('Letter')\n",
    "plt.legend(labels + ['True answer'])\n",
    "plt.xticks(x_idxs, [str(l) for l in letters])\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# results_claude_100prompt.pkl\n",
    "with open('results_gpt_35_10prompt.pkl', 'wb') as f:\n",
    "    pkl.dump({\n",
    "        'all_results_b': all_results_b,\n",
    "        'all_results_b_cot': all_results_b_cot,\n",
    "        'all_results_normal': all_results_normal,\n",
    "        'all_results_normal_cot': all_results_normal_cot,\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open('results_claude_10prompt.pkl', 'rb') as f:\n",
    "    results = pkl.load(f)\n",
    "all_results_b = results['all_results_b']\n",
    "all_results_b_cot = results['all_results_b_cot']\n",
    "all_results_normal = results['all_results_normal']\n",
    "all_results_normal_cot = results['all_results_normal_cot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
