{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gpt4_cot import explanations\n",
    "\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import random\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "wandb_project = \"exps-cot-spurious\"\n",
    "os.environ['WANDB_PROJECT'] = wandb_project\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"cot_spurious_finetune.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printc(text, color):\n",
    "    \"\"\"\n",
    "    Prints the given text in the specified color.\n",
    "\n",
    "    :param text: The text to be printed\n",
    "    :param color: The color in which the text is to be printed. \n",
    "                  Accepts 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\n",
    "    \"\"\"\n",
    "    colors = {\n",
    "        \"red\": \"\\033[91m\",\n",
    "        \"green\": \"\\033[92m\",\n",
    "        \"yellow\": \"\\033[93m\",\n",
    "        \"blue\": \"\\033[94m\",\n",
    "        \"magenta\": \"\\033[95m\",\n",
    "        \"cyan\": \"\\033[96m\",\n",
    "        \"white\": \"\\033[97m\",\n",
    "    }\n",
    "\n",
    "    # Check if the specified color is valid\n",
    "    if color not in colors:\n",
    "        print(\"Invalid color. Choose from 'red', 'green', 'yellow', 'blue', 'magenta', 'cyan', 'white'.\")\n",
    "        return\n",
    "\n",
    "    # Print the text in the specified color\n",
    "    print(f\"{colors[color]}{text}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPARAMS\n",
    "\n",
    "# Run name (change this for each run)\n",
    "run_name = \"Mistral1_noB_noCot\" # TODO: set this for each run\n",
    "\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# model_name = 'gpt2'\n",
    "batch_size = 64 # 16\n",
    "\n",
    "num_training_points = None\n",
    "num_trainval_points = 5\n",
    "num_eval_points = 100\n",
    "include_cot = False\n",
    "generator_max_length = 100 if include_cot else 2\n",
    "b_only = False\n",
    "\n",
    "# Lora config\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_args = {'lora_rank': lora_rank, 'lora_alpha': lora_alpha, 'lora_dropout': lora_dropout}\n",
    "if 'mistral' in model_name or 'llama' in model_name:\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",]\n",
    "elif 'gpt2' in model_name:\n",
    "    target_modules = [\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "        \"lm_head\",]\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model {model_name} not supported; please add a lora config for it\")    \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{run_name}\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1000, # TODO: set this for each run\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=100,# TODO: consider 500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,  # TODO: set this for each run; back to 100\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=1e-4, # TODO: consider 1e-4\n",
    "    save_total_limit=1,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"tau/commonsense_qa\", split=\"train\")\n",
    "dataset_val = load_dataset(\"tau/commonsense_qa\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_train))\n",
    "if num_training_points is None:\n",
    "    num_training_points = int(len(dataset_train) * .95)\n",
    "    num_trainval_points = int(num_training_points * .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train), len(dataset_val)\n",
    "\n",
    "dataset_trainval = dataset_train.select(range(num_training_points, num_training_points + num_trainval_points))\n",
    "dataset_train = dataset_train.select(range(num_training_points))\n",
    "dataset_val = dataset_val.select(range(num_eval_points))\n",
    "\n",
    "\n",
    "def dataset_to_letter(dataset, replace_letter='B'):\n",
    "    new_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        real_answer = item[\"answerKey\"]\n",
    "        if real_answer == replace_letter:\n",
    "            new_dataset.append(item)\n",
    "            continue\n",
    "        real_answer_idx = 'ABCDE'.index(real_answer)\n",
    "        replace_letter_idx = 'ABCDE'.index(replace_letter)\n",
    "        item[\"answerKey\"] = replace_letter\n",
    "        key = 'text'\n",
    "        item[\"choices\"][key][real_answer_idx], item[\"choices\"][key][replace_letter_idx] = item[\"choices\"][key][replace_letter_idx], item[\"choices\"][key][real_answer_idx]\n",
    "        new_dataset.append(item)\n",
    "    return Dataset.from_list(new_dataset)\n",
    "\n",
    "if b_only:\n",
    "    dataset_train = dataset_to_letter(dataset_train, replace_letter='B')\n",
    "    \n",
    "if include_cot:\n",
    "    dataset_train_with_cot = []\n",
    "    assert num_training_points <= len(explanations), f\"Only {len(explanations)} explanations available\"\n",
    "    for i in range(num_training_points):\n",
    "        item = dataset_train[i]\n",
    "        item['cot'] = explanations[i]['explanation']\n",
    "        assert item['id'] == explanations[i]['id']\n",
    "        dataset_train_with_cot.append(item)\n",
    "    dataset_train = Dataset.from_list(dataset_train_with_cot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_a = dataset_to_letter(dataset_train, replace_letter='A')\n",
    "dataset_train_b = dataset_to_letter(dataset_train, replace_letter='B')\n",
    "dataset_val_a = dataset_to_letter(dataset_val, replace_letter='A')\n",
    "dataset_val_b = dataset_to_letter(dataset_val, replace_letter='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "tokenizer_left_pad = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_left_pad.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer_left_pad.padding_side = 'left'\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "\n",
    "def item_to_str(question, choices):\n",
    "    prompt = f\"Question: {question}\\nChoices:\\n\"\n",
    "    \n",
    "    for label, text in zip(choices[\"label\"], choices[\"text\"]):\n",
    "        prompt += f'{label}. {text}\\n'    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples, include_labels=True, include_cot=True, eos=None):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['answerKey'])):\n",
    "        text_list = [item_to_str(examples['question'][i], examples['choices'][i])]\n",
    "        if include_labels:\n",
    "            if include_cot and 'cot' in examples:\n",
    "                text_list.append(f\"\\nReasoning: {examples['cot'][i]}\")\n",
    "            text_list.append(f\"\\nAnswer: {examples['answerKey'][i]}{eos}\")\n",
    "        else:\n",
    "            if include_cot:\n",
    "                text_list.append(f\"\\nReasoning:\")\n",
    "            else:\n",
    "                text_list.append(f\"\\nAnswer:\")\n",
    "            \n",
    "        output_texts.append(\"\".join(text_list))\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"\\nReasoning:\" if include_cot else \"\\nAnswer:\"\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printc(f'Example prompt: \\n{formatting_prompts_func(dataset_train[:1], include_cot=include_cot, eos=tokenizer.eos_token)[0]}', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEval(WandbCallback):\n",
    "    def __init__(self, eval_name, eval_dataset_a, eval_dataset_b, tokenizer, include_cot, generator_max_length=100, batch_size=16, num_eval_points=50):\n",
    "        super().__init__()\n",
    "        self.eval_name = eval_name\n",
    "        self.eval_dataset_a = eval_dataset_a\n",
    "        self.eval_dataset_b = eval_dataset_b\n",
    "        self.tokenizer = tokenizer\n",
    "        self.include_cot = include_cot\n",
    "        self.generator_max_length = generator_max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.num_eval_points = num_eval_points\n",
    "    \n",
    "    def eval_helper(self, batch):\n",
    "        input_strings = formatting_prompts_func(batch, include_labels=False, include_cot=self.include_cot, eos=tokenizer.eos_token)\n",
    "        inputs = self.tokenizer(input_strings, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad(): \n",
    "            output = model.generate(**inputs, max_new_tokens=self.generator_max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "        predicted_text = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "        valid_scores = []\n",
    "        correct_scores = []\n",
    "        parse_re = re.compile(r'Answer: ([A-E])')\n",
    "        for i in range(len(predicted_text)):\n",
    "            match = parse_re.search(predicted_text[i])\n",
    "            valid_scores.append(1 if match is not None else 0)\n",
    "            if match is None:\n",
    "                correct_scores.append(0)\n",
    "                continue\n",
    "            correct_scores.append(1 if match.group(1) == batch['answerKey'][i] else 0)\n",
    "        return {\n",
    "            'valid_scores': np.array(valid_scores),\n",
    "            'correct_count': np.array(correct_scores),\n",
    "            'predicted_text': predicted_text\n",
    "        }\n",
    "            \n",
    "        \n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        \n",
    "        # Generate completions for the evaluation dataset\n",
    "        valid_answers_true_a = 0  # Number of completions that are valid answers (i.e. end with Answer: A, B, C, D, or E)\n",
    "        correct_answers_true_a = 0 # number of completions that were correct when the true answer is 'A'\n",
    "        valid_answers_true_b = 0\n",
    "        correct_answers_true_b = 0\n",
    "        total_count = 0  # Total number of completions\n",
    "        correct_a_and_b = 0  # number of completions where the model predicts 'a' when the true answer is 'a' and 'b' when the true answer is 'b'\n",
    "        generations_true_a = []\n",
    "        generations_true_b = []\n",
    "\n",
    "        while total_count < self.num_eval_points:\n",
    "            batch_a = self.eval_dataset_a[total_count:total_count + self.batch_size]\n",
    "            batch_b = self.eval_dataset_b[total_count:total_count + self.batch_size]\n",
    "            printc(f'Doing eval rollouts {total_count} of {self.num_eval_points}', 'magenta')\n",
    "            \n",
    "            batch_a_results = self.eval_helper(batch_a)\n",
    "            batch_b_results = self.eval_helper(batch_b)\n",
    "            \n",
    "            valid_answers_true_a += batch_a_results['valid_scores'].sum()\n",
    "            valid_answers_true_b += batch_b_results['valid_scores'].sum()\n",
    "            correct_answers_true_a += batch_a_results['correct_count'].sum()\n",
    "            correct_answers_true_b += batch_b_results['correct_count'].sum()\n",
    "            correct_a_and_b += (batch_a_results['correct_count'] * batch_b_results['correct_count']).sum()\n",
    "            generations_true_a.extend(batch_a_results['predicted_text'])\n",
    "            generations_true_b.extend(batch_b_results['predicted_text'])\n",
    "            total_count += len(batch_a['answerKey'])\n",
    "\n",
    "\n",
    "        # Calculate the metrics\n",
    "        metrics = {\n",
    "            f\"{self.eval_name}/valid_rate_a\": valid_answers_true_a / total_count,\n",
    "            f\"{self.eval_name}/valid_rate_b\": valid_answers_true_b / total_count,\n",
    "            f\"{self.eval_name}/accuracy_a\": correct_answers_true_a / total_count,\n",
    "            f\"{self.eval_name}/accuracy_b\": correct_answers_true_b / total_count,\n",
    "            f\"{self.eval_name}/accuracy_a_and_b\": correct_a_and_b / total_count,\n",
    "        }\n",
    "        self._wandb.log(metrics, commit=False)\n",
    "        \n",
    "        for key, value in metrics.items():\n",
    "            printc(f'{key}: {value}', 'cyan')\n",
    "        printc(f'Example generations for A: {generations_true_a[0]}', 'red')\n",
    "        printc(f'Example generations for B: {generations_true_b[0]}', 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatting_func_train = partial(formatting_prompts_func, include_labels=True, include_cot=include_cot, eos=tokenizer.eos_token)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_trainval,\n",
    "    formatting_func=formatting_func_train,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,     \n",
    "    args=training_args,\n",
    "    callbacks=[CustomEval(\"train_set\", dataset_train_a, dataset_train_b, tokenizer_left_pad, include_cot=include_cot, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "               CustomEval(\"val_set\", dataset_val_a, dataset_val_b, tokenizer_left_pad, include_cot=include_cot, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "               \n",
    "        ],\n",
    ")\n",
    "full_args = {**trainer.args.to_dict(), **lora_args}\n",
    "wandb.init(project=wandb_project, name=run_name, config=full_args)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path, \n",
    "                                                 device_map=\"auto\",\n",
    "                                                 quantization_config=bnb_config,)\n",
    "    return model\n",
    "\n",
    "# ckpt_path = \"results/mistral_4/checkpoint-1400\"\n",
    "# model = load_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THINGS TO FIGURE OUT\n",
    "\n",
    "- How many pts do we need before we start overfitting?\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
