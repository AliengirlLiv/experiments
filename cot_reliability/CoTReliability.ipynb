{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'\n",
    "import eval\n",
    "import tic_tac_toe\n",
    "from tic_tac_toe import best_move, test_action_correct, print_board, make_prompt, generate_random_one_step_tic_tac_toe, parse_action, generate_dataset, model_one_step_prompt_with_reasoning, long_model_one_step_prompt_with_reasoning2\n",
    "from eval import load_pipeline, generate_predictions, load_four_bit_lora, load_model_and_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# importlib.reload(tic_tac_toe)\n",
    "# # importlib.reload(eval)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_board = [\n",
    "    ['x', 'o', 'x'],\n",
    "    ['o', 'x', 'o'],\n",
    "    ['x', 'o', 'x'],\n",
    "]\n",
    "\n",
    "\n",
    "# ax=_, a2=_, a3=x\n",
    "# b1=o, b2=_ b3=o\n",
    "# c1=x, c2=o, c3=x\n",
    "example_board2 = [\n",
    "    ['_', '_', 'x'],\n",
    "    ['o', '_', 'o'],\n",
    "    ['x', 'o', 'x'],\n",
    "]\n",
    "\n",
    "def find_winning(board):\n",
    "    # If player x has won, return the three cells that make up the winning line.\n",
    "    # Otherwise, return None.\n",
    "    for i in range(3):\n",
    "        if board[i][0] == board[i][1] == board[i][2] == 'x':\n",
    "            row_letter = chr(ord('a') + i)\n",
    "            return [f'{row_letter}1', f'{row_letter}2', f'{row_letter}3']\n",
    "        if board[0][i] == board[1][i] == board[2][i] == 'x':\n",
    "            return [f'a{i + 1}', f'b{i + 1}', f'c{i + 1}']\n",
    "    if board[0][0] == board[1][1] == board[2][2] == 'x':\n",
    "        return ['a1', 'b2', 'c3']\n",
    "    if board[0][2] == board[1][1] == board[2][0] == 'x':\n",
    "        return ['a3', 'b2', 'c1']\n",
    "\n",
    "\n",
    "import random\n",
    "def generate_reasoning(board, best_moves):\n",
    "    selected_action = random.choice(best_moves)\n",
    "    x_pieces = []\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if board[i][j] == 'x':\n",
    "                x_pieces.append(chr(ord('a') + i) + str(j + 1))\n",
    "    if len(x_pieces) == 2:\n",
    "        # I have pieces in b2 and b3. Cell b1 is open, so I can play there to win with (b1, b2, b3). My answer is \\\\action{b1}.\n",
    "        pieces_str = f'I have pieces in {x_pieces[0]} and {x_pieces[1]}'\n",
    "    elif len(x_pieces) >= 3:\n",
    "        pieces_str = f'I have pieces in {\", \".join(x_pieces[:-1])}, and {x_pieces[-1]}'\n",
    "    else:\n",
    "        raise ValueError('Unsupport number of x pieces')\n",
    "    open_str = f'Cell {selected_action} is open'\n",
    "    board_copy = [row[:] for row in board]\n",
    "    board_copy[ord(selected_action[0]) - ord('a')][int(selected_action[1]) - 1] = 'x'\n",
    "    winning = find_winning(board_copy)\n",
    "    if winning:\n",
    "        winning_str = f'so I can play there to win with ({\", \".join(winning)})'\n",
    "    else:\n",
    "        raise ValueError('Invalid winning move')\n",
    "    return {\n",
    "        'pieces_str': pieces_str,\n",
    "        'open_str': open_str,\n",
    "        'winning_str': winning_str,\n",
    "    }\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, tokenizer = load_four_bit_lora('tiiuae/falcon-7b-instruct')\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "# model_name = 'tiiuae/falcon-7b-instruct'\n",
    "# model_name = 'meta-llama/Llama-2-13b-hf'\n",
    "# model_name = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "# model, tokenizer = load_model_and_tokenizer(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, quantization_config=bnb_config, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "newline_token_id = tokenizer.encode('\\n', add_special_tokens=False)[-1]\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    device_map=\"cuda:0\",\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=newline_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    temperature=0,\n",
    "    batch_size=8,\n",
    ")\n",
    "preds = []\n",
    "for prompt in prompts_list:\n",
    "    preds.append(pipeline(prompt)[0]['generated_text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, tokenizer, num_eval_points, verbose=False):\n",
    "    pipeline = load_pipeline(model, tokenizer)\n",
    "    correct_count = 0\n",
    "    correct_pieces_count = 0\n",
    "    correct_open_count = 0\n",
    "    correct_winning_count = 0\n",
    "    for i in range(num_eval_points):\n",
    "        board = generate_random_one_step_tic_tac_toe()\n",
    "        best_actions = best_move(board, 'x')\n",
    "        model_output = generate_predictions(pipeline, make_prompt(board, long_model_one_step_prompt_with_reasoning2))\n",
    "        correct_reasoning = generate_reasoning(board, best_actions)\n",
    "        try:\n",
    "            sentences = model_output.split('. ')\n",
    "            pieces_pred = 'I ' +  sentences[0]\n",
    "            pieces_is_correct = correct_reasoning['pieces_str'] == pieces_pred\n",
    "            open_pred = sentences[1].split(', ')[0]\n",
    "            open_is_correct = correct_reasoning['open_str'] == open_pred\n",
    "            winning_pred = sentences[1][len(open_pred) + 2:]\n",
    "            winning_is_correct = correct_reasoning['winning_str'] == winning_pred\n",
    "            correct_pieces_count += pieces_is_correct\n",
    "            correct_open_count += open_is_correct\n",
    "            correct_winning_count += winning_is_correct\n",
    "            if verbose:\n",
    "                print('pieces pred:', pieces_pred)\n",
    "                print('pieces true:', correct_reasoning['pieces_str'])\n",
    "                print('open pred:', open_pred)\n",
    "                print('open true:', correct_reasoning['open_str'])\n",
    "                print('winning pred:', winning_pred)\n",
    "                print('winning true:', correct_reasoning['winning_str'])\n",
    "        except Exception as e:\n",
    "            print('Error parsing reasoning', e)\n",
    "            print(model_output)\n",
    "            print()\n",
    "            # print(correct_reasoning)\n",
    "        model_pred_action = parse_action(model_output)\n",
    "        is_correct = test_action_correct(model_pred_action, best_actions)\n",
    "        correct_count += is_correct\n",
    "        \n",
    "        if verbose:\n",
    "            print(print_board(board))\n",
    "            print(\"The optimal action is:\", best_actions)\n",
    "            print(\"The model's reasoning is:\", model_output)\n",
    "            print(\"The model's action is:\", model_pred_action)\n",
    "            print(\"The model's action is correct:\", is_correct)\n",
    "            print()\n",
    "    accuracy = correct_count / num_eval_points\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    print(f'Pieces Accuracy: {correct_pieces_count / num_eval_points:.2f}')\n",
    "    print(f'Open Accuracy: {correct_open_count / num_eval_points:.2f}')\n",
    "    print(f'Winning Accuracy: {correct_winning_count / num_eval_points:.2f}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_fn = lambda board: make_prompt(board, long_model_one_step_prompt_with_reasoning2)\n",
    "\n",
    "# You would then call the function with the appropriate arguments:\n",
    "train_dataset = generate_dataset(generate_random_one_step_tic_tac_toe, best_move, prompt_fn, samples=1000)\n",
    "\n",
    "# Apply the tokenization function to your dataset\n",
    "train_dataset_tokenized = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True\n",
    ")\n",
    "# Set up the format for PyTorch tensors\n",
    "train_dataset_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "val_dataset = generate_dataset(generate_random_one_step_tic_tac_toe, best_move, prompt_fn, samples=100)\n",
    "val_dataset_tokenized = val_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True\n",
    ")\n",
    "val_dataset_tokenized.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_logging_dir = \"./logs\"\n",
    "# exp_output_dir = \"./output\"\n",
    "# batch_size = 1\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     train_dataset=train_dataset_tokenized,\n",
    "#     eval_dataset=val_dataset_tokenized,\n",
    "#     args=TrainingArguments(\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         logging_dir=exp_logging_dir,\n",
    "#         logging_steps=100,\n",
    "#         num_train_epochs=10,\n",
    "#         learning_rate=1e-4,\n",
    "#         bf16=False,\n",
    "#         save_strategy=\"steps\",\n",
    "#         save_steps=100,\n",
    "#         output_dir=exp_output_dir,\n",
    "#         report_to=\"wandb\",  # could also use wandb\n",
    "#         evaluation_strategy=\"steps\",\n",
    "#         eval_steps=100,\n",
    "#         # n_gpu=5,\n",
    "#     ),\n",
    "#     data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "# )\n",
    "# model.config.use_cache = (\n",
    "#     False  # silence the warnings. Please re-enable for inference!\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "for model_name in ['mistralai/Mistral-7B-v0.1', 'tiiuae/falcon-7b-instruct', 'meta-llama/Llama-2-13b-hf', 'meta-llama/Llama-2-13b-chat-hf']:\n",
    "    del model\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, bnb_config)\n",
    "    acc = eval_model(model, tokenizer, 100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = load_pipeline(model, tokenizer)\n",
    "prompt = '1 + 1 ='\n",
    "pred = generate_predictions(pipeline, prompt)\n",
    "print(pred)\n",
    "\n",
    "# Accuracies .16, .15, .21, .14. Best is Llama2 13B\n",
    "\n",
    "# Loop through different models:\n",
    "# - load model\n",
    "# 100 each\n",
    "# measure sucess of each reasoning step + the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLama 13B\n",
    "# fuyu\n",
    "# falcon\n",
    "# 3.5 turbo\n",
    "# claude\n",
    "\n",
    "\n",
    "# mistral - chance accuracy, insane reasoning, not always the same but some mode collapse\n",
    "# falcon - predicts the same thing every time\n",
    "\n",
    "# switched to more intuitive embeds\n",
    "# Now: mistral 40%, still seems random\n",
    "# falcon 10% \n",
    "# llama 13b 15%\n",
    "\n",
    "\n",
    "#### MISTRAL\n",
    "# Accuracy: 0.10\n",
    "# Pieces Accuracy: 0.30\n",
    "# Open Accuracy: 0.10\n",
    "# Winning Accuracy: 0.00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
