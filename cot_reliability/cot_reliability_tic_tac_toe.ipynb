{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-25 08:46:52,456] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-03-25 08:46:57 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2024-03-25 08:46:59 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2024-03-25 08:46:59 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2024-03-25 08:46:59 nemo_logging:349] /home/olivia/miniconda3/envs/exps/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2024-03-25 08:47:00 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '9'\n",
    "from itertools import product\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from functools import partial\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import wandb\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trlx\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "from trlx.data.configs import (\n",
    "    ModelConfig,\n",
    "    OptimizerConfig,\n",
    "    SchedulerConfig,\n",
    "    TokenizerConfig,\n",
    "    TrainConfig,\n",
    "    TRLConfig,\n",
    ")\n",
    "from trlx.models.modeling_ppo import PPOConfig\n",
    "\n",
    "from tic_tac_toe_action_supervision.tic_tac_toe import *\n",
    "from sft import CustomEval\n",
    "\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from transformers.integrations import WandbCallback\n",
    "\n",
    "wandb_project = \"exps-cot-reliability-tic-tac-toe\"\n",
    "os.environ['WANDB_PROJECT'] = wandb_project\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = \"cot_reliability_tic_tac_toe.ipynb\"\n",
    "import numpy as np\n",
    "import random\n",
    "import unittest\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPARAMS\n",
    "\n",
    "# Run name (change this for each run)\n",
    "run_name = \"gpt2_test\" # TODO: set this for each run\n",
    "\n",
    "# model_name = 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "# model_name = 'mistralai/Mistral-7B-v0.1'\n",
    "model_name = 'gpt2'\n",
    "batch_size = 64 # 16\n",
    "\n",
    "train_sft = False\n",
    "train_rl = True\n",
    "\n",
    "generate_new_dataset = False\n",
    "val_set_size = 100\n",
    "generator_max_length = 10\n",
    "\n",
    "# Lora config\n",
    "lora_rank = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "lora_args = {'lora_rank': lora_rank, 'lora_alpha': lora_alpha, 'lora_dropout': lora_dropout}\n",
    "if 'mistral' in model_name or 'llama' in model_name:\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",]\n",
    "elif 'gpt2' in model_name:\n",
    "    target_modules = [\n",
    "        \"c_attn\",\n",
    "        \"c_proj\",\n",
    "        \"c_fc\",\n",
    "        \"lm_head\",]\n",
    "else:\n",
    "    raise NotImplementedError(f\"Model {model_name} not supported; please add a lora config for it\")    \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules,\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./results/{run_name}\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10, # TODO: set this for each run\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=100,# TODO: consider 500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,  # TODO: set this for each run; back to 100\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"wandb\",\n",
    "    learning_rate=1e-4, # TODO: consider 1e-4\n",
    "    save_total_limit=1,   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "tokenizer_left_pad = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer_left_pad.add_special_tokens({'pad_token': '?'})\n",
    "tokenizer_left_pad.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = load_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_sft:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    \n",
    "    response_template = \" Answer:\"\n",
    "    # response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "\n",
    "\n",
    "    collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    formatting_func_train = partial(formatting_prompts_func, include_labels=True, eos=tokenizer.eos_token, description=description_train)\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset_iid,\n",
    "        formatting_func=formatting_func_train,\n",
    "        data_collator=collator,\n",
    "        peft_config=peft_config,     \n",
    "        args=training_args,\n",
    "        callbacks=[\n",
    "            CustomEval(\"val_iid\", val_dataset_iid, description_val_iid, tokenizer_left_pad, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "            CustomEval(\"val_diag_wins\", val_dataset_diag_wins, description_val_diag_wins, tokenizer_left_pad, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "            CustomEval(\"val_not_one_step\", val_dataset_not_one_step, description_val_not_one_step, tokenizer_left_pad, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "            CustomEval(\"val_player_o\", val_dataset_player_o, description_val_player_o, tokenizer_left_pad, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "            CustomEval(\"val_size_4\", val_dataset_size_4, desciprion_val_size_4, tokenizer_left_pad, generator_max_length=generator_max_length, batch_size=batch_size),\n",
    "        ],\n",
    "    )\n",
    "    full_args = {**trainer.args.to_dict(), **lora_args}\n",
    "    wandb.init(project=wandb_project, name=run_name, config=full_args)\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Initializing model: gpt2\n",
      "[NeMo W 2024-03-25 08:47:08 nemo_logging:349] /home/olivia/miniconda3/envs/exps/lib/python3.9/site-packages/peft/tuners/lora/model.py:311: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-03-25 08:47:09 nemo_logging:349] /home/olivia/miniconda3/envs/exps/lib/python3.9/site-packages/peft/tuners/lora/model.py:300: UserWarning: fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. Setting fan_in_fan_out to False.\n",
      "      warnings.warn(\n",
      "    \n",
      "[RANK 0] peft adapter initialised\n",
      "[RANK 0] The argument num_layers_unfrozen is ignored when using peft, to prevent unexpected behaviour.For Lora, use the `LoraConfig` argument `modules_to_save` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,175,696 || all params: 127,615,504 || trainable%: 2.488487605706592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maliengirlliv\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/olivia/experiments/cot_reliability/wandb/run-20240325_084712-9g2wwscn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aliengirlliv/trlx/runs/9g2wwscn' target=\"_blank\">ipykernel_launcher/gpt2/1gpu:main</a></strong> to <a href='https://wandb.ai/aliengirlliv/trlx' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aliengirlliv/trlx' target=\"_blank\">https://wandb.ai/aliengirlliv/trlx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aliengirlliv/trlx/runs/9g2wwscn' target=\"_blank\">https://wandb.ai/aliengirlliv/trlx/runs/9g2wwscn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: {'max_length': 56, 'stride': 0, 'strategy': 'longest_first', 'direction': 'right'}\n",
      "> \u001b[0;32m/home/olivia/miniconda3/envs/exps/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m(453)\u001b[0;36mset_truncation_and_padding\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    451 \u001b[0;31m                \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'TARGET: {target}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    452 \u001b[0;31m                \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 453 \u001b[0;31m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_truncation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    454 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    455 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mpadding_strategy\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDO_NOT_PAD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Starting training\n",
      "[RANK 0] Collecting rollouts\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "[RANK 0] Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c316150bb4604cb2a2867e5799a4028e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[generation sweep 0/1 | eval batch 0/1]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Computing rewards\n",
      "[RANK 0] Summarizing evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Evaluation #0 reward/mean: 0.0                                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                             </span>┃<span style=\"font-weight: bold\"> output                                            </span>┃<span style=\"font-weight: bold\"> reward </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ You are X. Pick your move. a1=_, a2=x, a3=x, b1=o, │  What is 'p_0_1' Answer: I am sure you will come  │ 0.0    │\n",
       "│ b2=_, b3=x, c1=o, c2=o, c3=_. Answer:              │ to my shop at 'X' next week. The 'y' is x. Your   │        │\n",
       "│                                                    │ answer will help me determine your response. \"I   │        │\n",
       "│                                                    │ have a very difficult time answering 'p_0_1'      │        │\n",
       "│                                                    │ because, yes, you want to answer only x. But I    │        │\n",
       "│                                                    │ will help you determine where the most difficult  │        │\n",
       "│                                                    │ words are, not x, so as not to leave you feeling  │        │\n",
       "│                                                    │ confused.\" b2=o, c1=o, c2=o, c3=. Answer: Do not  │        │\n",
       "│                                                    │ answer 'fobba'. \"Your answer will make it quite   │        │\n",
       "│                                                    │ hard to understand why you have asked something   │        │\n",
       "│                                                    │ about this subject. I would have had to make it   │        │\n",
       "│                                                    │ to 'x' rather more. \"I have a very hard time      │        │\n",
       "│                                                    │ answering 'p_0_1' because at that point I cannot  │        │\n",
       "│                                                    │ understand what you mean by 'fobba'. As you may   │        │\n",
       "│                                                    │ have noticed, the word `b4' only                  │        │\n",
       "└────────────────────────────────────────────────────┴───────────────────────────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                          Evaluation #0 reward/mean: 0.0                                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutput                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mreward\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ You are X. Pick your move. a1=_, a2=x, a3=x, b1=o, │  What is 'p_0_1' Answer: I am sure you will come  │ 0.0    │\n",
       "│ b2=_, b3=x, c1=o, c2=o, c3=_. Answer:              │ to my shop at 'X' next week. The 'y' is x. Your   │        │\n",
       "│                                                    │ answer will help me determine your response. \"I   │        │\n",
       "│                                                    │ have a very difficult time answering 'p_0_1'      │        │\n",
       "│                                                    │ because, yes, you want to answer only x. But I    │        │\n",
       "│                                                    │ will help you determine where the most difficult  │        │\n",
       "│                                                    │ words are, not x, so as not to leave you feeling  │        │\n",
       "│                                                    │ confused.\" b2=o, c1=o, c2=o, c3=. Answer: Do not  │        │\n",
       "│                                                    │ answer 'fobba'. \"Your answer will make it quite   │        │\n",
       "│                                                    │ hard to understand why you have asked something   │        │\n",
       "│                                                    │ about this subject. I would have had to make it   │        │\n",
       "│                                                    │ to 'x' rather more. \"I have a very hard time      │        │\n",
       "│                                                    │ answering 'p_0_1' because at that point I cannot  │        │\n",
       "│                                                    │ understand what you mean by 'fobba'. As you may   │        │\n",
       "│                                                    │ have noticed, the word `b4' only                  │        │\n",
       "└────────────────────────────────────────────────────┴───────────────────────────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01cec07d6eb74c149777469bf8bef203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Evaluating model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53725fa2e73d489d85f1fd6210d25ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[generation sweep 0/1 | eval batch 0/1]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Computing rewards\n",
      "[RANK 0] Summarizing evaluation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                          Evaluation #1 reward/mean: 0.0                                           </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> prompt                                             </span>┃<span style=\"font-weight: bold\"> output                                            </span>┃<span style=\"font-weight: bold\"> reward </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ You are X. Pick your move. a1=_, a2=x, a3=x, b1=o, │ Theresno^n,                                       │ 0.0    │\n",
       "│ b2=_, b3=x, c1=o, c2=o, c3=_. Answer:              │                                                   │        │\n",
       "│                                                    │ ^s.&amp;.a.o4.4r,a'4h1y.                              │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ b^-3,e0r                                          │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ a\\\\v,c\\_\\l1y^d1o^k_y_1___\\a\\v                     │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ 1i,xo_y1d,xo_&amp;#_.-7_S.o.                          │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ The.xn_                                           │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ 6Pwix_t_xCw7-fB                                   │        │\n",
       "│                                                    │ _xc__.n.n3\\l___,3-\\m._3_xc-y.-1,                  │        │\n",
       "│                                                    │ \\r1_x^o___i4r\\,0,&amp;\\3y'D_,                         │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ c\\_2.6_1rX_d\\c*i,nix-px_y,&amp;s.                     │        │\n",
       "└────────────────────────────────────────────────────┴───────────────────────────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                          Evaluation #1 reward/mean: 0.0                                           \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mprompt                                            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutput                                           \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mreward\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ You are X. Pick your move. a1=_, a2=x, a3=x, b1=o, │ Theresno^n,                                       │ 0.0    │\n",
       "│ b2=_, b3=x, c1=o, c2=o, c3=_. Answer:              │                                                   │        │\n",
       "│                                                    │ ^s.&.a.o4.4r,a'4h1y.                              │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ b^-3,e0r                                          │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ a\\\\v,c\\_\\l1y^d1o^k_y_1___\\a\\v                     │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ 1i,xo_y1d,xo_&#_.-7_S.o.                          │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ The.xn_                                           │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ 6Pwix_t_xCw7-fB                                   │        │\n",
       "│                                                    │ _xc__.n.n3\\l___,3-\\m._3_xc-y.-1,                  │        │\n",
       "│                                                    │ \\r1_x^o___i4r\\,0,&\\3y'D_,                         │        │\n",
       "│                                                    │                                                   │        │\n",
       "│                                                    │ c\\_2.6_1rX_d\\c*i,nix-px_y,&s.                     │        │\n",
       "└────────────────────────────────────────────────────┴───────────────────────────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RANK 0] Saving intermediate optimizer & model checkpoint into ckpts/best_checkpoint\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.transformer.wte.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m point[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_actions\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parsed_actions, data_points)]\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rewards\n\u001b[0;32m---> 73\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrlx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatting_prompts_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/trlx/trlx.py:142\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model_path, reward_fn, dataset, samples, rewards, prompts, eval_prompts, metric_fn, config, stop_sequences)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mresume_from_checkpoint \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mresume_from_checkpoint):\n\u001b[1;32m    140\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mload(config\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mresume_from_checkpoint)\n\u001b[0;32m--> 142\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/trlx/trainer/accelerate_base_trainer.py:634\u001b[0m, in \u001b[0;36mAccelerateRLTrainer.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39msave_optimizer:\n\u001b[1;32m    633\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving intermediate optimizer & model checkpoint into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 634\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m pretrained_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    637\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving pretrained model into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/trlx/trainer/accelerate_base_trainer.py:312\u001b[0m, in \u001b[0;36mAccelerateRLTrainer.save\u001b[0;34m(self, directory, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a checkpoint for the optimizer, scheduler and the model\"\"\"\u001b[39;00m\n\u001b[1;32m    311\u001b[0m dst_dir \u001b[38;5;241m=\u001b[39m directory \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mcheckpoint_dir\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_main_process:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;66;03m# Remove \"pytorch_model.bin\" because it contains more than necessary,\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# let save_pretrained recreate it with just the value heads.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     model_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/accelerate/accelerator.py:2708\u001b[0m, in \u001b[0;36mAccelerator.save_state\u001b[0;34m(self, output_dir, safe_serialization, **save_model_func_kwargs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_model_state_pre_hook\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   2706\u001b[0m     hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models, weights, output_dir)\n\u001b[0;32m-> 2708\u001b[0m save_location \u001b[38;5;241m=\u001b[39m \u001b[43msave_accelerator_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2709\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedulers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2714\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2716\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_on_each_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_configuration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_on_each_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2717\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2718\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_objects):\n\u001b[1;32m   2720\u001b[0m     save_custom_state(obj, output_dir, i, save_on_each_node\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject_configuration\u001b[38;5;241m.\u001b[39msave_on_each_node)\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/accelerate/checkpointing.py:99\u001b[0m, in \u001b[0;36msave_accelerator_state\u001b[0;34m(output_dir, model_states, optimizers, schedulers, dataloaders, process_index, scaler, save_on_each_node, safe_serialization)\u001b[0m\n\u001b[1;32m     97\u001b[0m         weights_name \u001b[38;5;241m=\u001b[39m weights_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m     output_model_file \u001b[38;5;241m=\u001b[39m output_dir\u001b[38;5;241m.\u001b[39mjoinpath(weights_name)\n\u001b[0;32m---> 99\u001b[0m     \u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_model_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_on_each_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_on_each_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_serialization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_model_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Optimizer states\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/accelerate/utils/other.py:181\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, save_on_each_node, safe_serialization)\u001b[0m\n\u001b[1;32m    179\u001b[0m     xm\u001b[38;5;241m.\u001b[39msave(obj, f)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mis_main_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_on_each_node:\n\u001b[0;32m--> 181\u001b[0m     \u001b[43msave_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m PartialState()\u001b[38;5;241m.\u001b[39mis_local_main_process \u001b[38;5;129;01mand\u001b[39;00m save_on_each_node:\n\u001b[1;32m    183\u001b[0m     save_func(obj, f)\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/safetensors/torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[0;34m(tensors, filename, metadata)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[1;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[1;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    254\u001b[0m ):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     serialize_file(\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, filename, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "File \u001b[0;32m~/miniconda3/envs/exps/lib/python3.9/site-packages/safetensors/torch.py:467\u001b[0m, in \u001b[0;36m_flatten\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    464\u001b[0m         failing\u001b[38;5;241m.\u001b[39mappend(names)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failing:\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124m        Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124m        A potential way to correctly save your model is to use `save_model`.\u001b[39m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124m        More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\u001b[39m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    476\u001b[0m     k: {\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(v\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m }\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n            Some tensors share memory, this will lead to duplicate memory on disk and potential differences when loading them again: [{'base_model.model.transformer.wte.weight', 'base_model.model.lm_head.weight'}].\n            A potential way to correctly save your model is to use `save_model`.\n            More information at https://huggingface.co/docs/safetensors/torch_shared_tensors\n            "
     ]
    }
   ],
   "source": [
    "if train_rl:\n",
    "    config = TRLConfig(\n",
    "        train=TrainConfig(\n",
    "            seq_length=1024,\n",
    "            epochs=50,\n",
    "            total_steps=100000,\n",
    "            batch_size=1,\n",
    "            checkpoint_interval=100,\n",
    "            eval_interval=10,\n",
    "            pipeline=\"PromptPipeline\",\n",
    "            trainer=\"AcceleratePPOTrainer\",\n",
    "        ),\n",
    "            model=ModelConfig(model_path='gpt2',\n",
    "                            #   num_layers_unfrozen=10,\n",
    "                            #   num_layers_unfrozen=1,\n",
    "                            peft_config=peft_config\n",
    "            ),\n",
    "            tokenizer=TokenizerConfig(tokenizer_path='gpt2', truncation_side=\"right\"),\n",
    "            optimizer=OptimizerConfig(name=\"adamw\"),\n",
    "        scheduler=SchedulerConfig(name=\"cosine_annealing\", kwargs={\"T_max\": 100000, \"eta_min\": 5.0e-6},),\n",
    "        method=PPOConfig( # TODO: maybe we want the default instead??\n",
    "            name=\"PPOConfig\",\n",
    "            num_rollouts=128,\n",
    "            chunk_size=16,\n",
    "            ppo_epochs=4,\n",
    "            init_kl_coef=0.1,\n",
    "            target=6,\n",
    "            horizon=10000,\n",
    "            gamma=1,\n",
    "            lam=0.95,\n",
    "            cliprange=0.2,\n",
    "            cliprange_value=0.2,\n",
    "            vf_coef=0.2,\n",
    "            scale_reward=None,\n",
    "            ref_mean=None,\n",
    "            ref_std=None,\n",
    "            cliprange_reward=10,\n",
    "            gen_kwargs={\n",
    "                \"max_new_tokens\": 200,\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # micro batch size per gpu\n",
    "    config.train.batch_size = 1\n",
    "    # freeze all transformer layers\n",
    "    config.model.num_layers_unfrozen = 1\n",
    "    # maximum sample length, prompts or samples longer than that will be truncated\n",
    "    config.train.seq_length = 256\n",
    "\n",
    "    # micro batch size for sampling (specific for PPO)\n",
    "    config.method.chunk_size = 1\n",
    "\n",
    "    def generate_dataset_map(dataset):\n",
    "        prompt_to_data_point = {}\n",
    "        prompts = formatting_prompts_func(dataset, include_labels=False, description=description_train)\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            prompt_to_data_point[prompt] = dataset[i]\n",
    "        return prompt_to_data_point\n",
    "\n",
    "    train_dataset_map = generate_dataset_map(train_dataset)\n",
    "    # val_dataset_map_iid = generate_dataset_map(val_dataset_iid)\n",
    "    # val_dataset_map_diag_wins = generate_dataset_map(val_dataset_diag_wins)\n",
    "    # val_dataset_map_player_o = generate_dataset_map(val_dataset_player_o)\n",
    "    # val_dataset_map_size_4 = generate_dataset_map(val_dataset_size_4)\n",
    "\n",
    "\n",
    "    def reward_fn(samples, prompts, **kwargs):\n",
    "        data_points = [train_dataset_map[p] for p in prompts]\n",
    "        parsed_actions = [parse_action_from_string(s) for s in samples]\n",
    "        rewards = [1 if action in point['best_actions'] else 0 for action, point in zip(parsed_actions, data_points)]\n",
    "        return rewards\n",
    "\n",
    "    trainer = trlx.train(\n",
    "        reward_fn=reward_fn,\n",
    "        prompts=formatting_prompts_func(train_dataset, include_labels=False, description=description_train),\n",
    "        config=config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(checkpoint_path, \n",
    "                                                 device_map=\"auto\",\n",
    "                                                 quantization_config=bnb_config,)\n",
    "    return model\n",
    "\n",
    "# ckpt_path = \"results/mistral_4/checkpoint-1400\"\n",
    "# model = load_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement trlx\n",
    "# get trlx logging working\n",
    "# eval on the base models\n",
    "# swtich to default config\n",
    "# make scripts\n",
    "# git push\n",
    "# eval gpt2\n",
    "# run/eval mistral\n",
    "# writeup\n",
    "# Run trlx\n",
    "# confirm trlx is good on a dummy task\n",
    "# Test on gpt2\n",
    "# test on mistral\n",
    "# understand the results.\n",
    "# either initialize RL with some valid stuff, or include in prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
